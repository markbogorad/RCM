{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba9512",
   "metadata": {},
   "source": [
    "# Dakota vs Salesforce Data Clean\n",
    "- USA name standardization done for both in load block\n",
    "- Map state ownership for both\n",
    "\n",
    "## Dakota Clean Up\n",
    "Raw | Wirehouses | Blank Emails, Dupe Emails, Dupe Names | Clean | AUM Aggregated\n",
    "\n",
    "# Salesforce Clean Up\n",
    "Raw | Dupe Emails | Dupe Names | Empty Emails | Clean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fae72d",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60deb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (2.3.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (3.1.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl matplotlib \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import difflib\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1568a",
   "metadata": {},
   "source": [
    "# Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfd2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dakota_df=pd.read_excel(\"../data/all_contacts/Dakota_All_Contacts.xlsx\")\n",
    "matched_contacts=pd.read_excel(\"../data/prospect/matched_contacts.xlsx\")\n",
    "matched_accounts=pd.read_excel(\"../data/prospect/matched_accounts.xlsx\")\n",
    "unmatched_contacts=pd.read_excel(\"../data/prospect/unmatched_contacts.xlsx\")\n",
    "unmatched_accounts=pd.read_excel(\"../data/prospect/unmatched_accounts.xlsx\")\n",
    "\n",
    "salesforce_df = pd.read_excel(\"../data/all_contacts/Salesforce_dataset.xlsx\") # Josh approved\n",
    "salesforce_618 = pd.read_excel(\"../data/all_contacts/Contact_SF_Extract_6_16_25.xlsx\") # find out difference between this and the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f24dfd",
   "metadata": {},
   "source": [
    "# Mutual Changes\n",
    "- U.S Standardization\n",
    "- Ownership Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb65bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standardization function\n",
    "def standardize_country(country, state):\n",
    "    if pd.isnull(country):\n",
    "        country = ''\n",
    "    country_upper = str(country).strip().upper()\n",
    "    if country_upper in ['UNITED STATES', 'U.S.', 'USA', 'US']:\n",
    "        return 'United States'\n",
    "    if pd.notnull(state) and str(state).strip() != '':\n",
    "        return 'United States'\n",
    "    return country\n",
    "\n",
    "# List of possible column names for country and state\n",
    "country_column_candidates = [\n",
    "    'Mailing Country', 'MailingCountry', 'Billing Country', 'BillingCountry',\n",
    "    'Provided BillingCountry', 'Dakota Billing Country', 'Provided MailingCountry', 'Dakota Mailing Country'\n",
    "]\n",
    "state_column_candidates = [\n",
    "    'Mailing State/Province', 'Mailing State', 'Billing State/Province', 'Billing State'\n",
    "]\n",
    "\n",
    "# Function to find the first matching column from a list of candidates\n",
    "def find_column(df, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# Apply standardization to a list of DataFrames\n",
    "def apply_country_standardization(dfs):\n",
    "    for df in dfs:\n",
    "        country_col = find_column(df, country_column_candidates)\n",
    "        state_col = find_column(df, state_column_candidates)\n",
    "        if country_col:\n",
    "            df['Mailing Country'] = df.apply(\n",
    "                lambda row: standardize_country(row.get(country_col), row.get(state_col)),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "# Example usage:\n",
    "apply_country_standardization([dakota_df, matched_contacts, matched_accounts, unmatched_contacts, unmatched_accounts, salesforce_df, salesforce_618])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eb988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state → team member\n",
    "state_team_map = {\n",
    "    'AL': 'AG', 'AK': 'Rockefeller Asset Management', 'AZ': 'KH', 'AR': 'Rockefeller Asset Management',\n",
    "    'CA': 'AP', 'CO': 'KH', 'CT': 'Rockefeller Asset Management', 'DE': 'KH', 'FL': 'Rockefeller Asset Management',\n",
    "    'GA': 'AP', 'HI': 'Rockefeller Asset Management', 'ID': 'Rockefeller Asset Management', 'IL': 'MJ',\n",
    "    'IN': 'KH', 'IA': 'Rockefeller Asset Management', 'KS': 'Rockefeller Asset Management', 'KY': 'AG',\n",
    "    'LA': 'Rockefeller Asset Management', 'ME': 'Rockefeller Asset Management', 'MD': 'KH', 'MA': 'AP',\n",
    "    'MI': 'KH', 'MN': 'Rockefeller Asset Management', 'MS': 'Rockefeller Asset Management', 'MO': 'AG',\n",
    "    'MT': 'Rockefeller Asset Management', 'NE': 'Rockefeller Asset Management', 'NV': 'Rockefeller Asset Management',\n",
    "    'NH': 'Rockefeller Asset Management', 'NJ': 'KH', 'NM': 'KH', 'NY': 'AG', 'NC': 'AG', 'ND': 'Rockefeller Asset Management',\n",
    "    'OH': 'KH', 'OK': 'Rockefeller Asset Management', 'OR': 'Rockefeller Asset Management', 'PA': 'KH',\n",
    "    'RI': 'Rockefeller Asset Management', 'SC': 'AG', 'SD': 'Rockefeller Asset Management', 'TN': 'AG',\n",
    "    'TX': 'MJ', 'UT': 'KH', 'VT': 'Rockefeller Asset Management', 'VA': 'KH', 'WA': 'Rockefeller Asset Management',\n",
    "    'WV': 'Rockefeller Asset Management', 'WI': 'MJ', 'WY': 'Rockefeller Asset Management', 'DC': 'KH'\n",
    "}\n",
    "\n",
    "# Define team member → OwnerID\n",
    "team_ownerID_map = {\n",
    "    'AG': '005a600000177CHAAY',\n",
    "    'Rockefeller Asset Management': '005a6000005CXtNAAW',\n",
    "    'KH': '005a6000000c1j8AAA',\n",
    "    'AP': '005a6000000c1jDAAQ',\n",
    "    'MJ': '005a60000043nujAAA'\n",
    "}\n",
    "\n",
    "# Example: apply to a list of dataframes\n",
    "dataframes = {\n",
    "    \"dakota_df\": dakota_df,\n",
    "    \"matched_contacts\": matched_contacts,\n",
    "    \"matched_accounts\": matched_accounts,\n",
    "    \"unmatched_contacts\": unmatched_contacts,\n",
    "    \"unmatched_accounts\": unmatched_accounts,\n",
    "    \"salesforce_df\": salesforce_df,\n",
    "    \"salesforce_618\": salesforce_618\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    if 'Company Owner' not in df.columns:\n",
    "        df['Company Owner'] = None\n",
    "    if 'Mailing State/Province' in df.columns:\n",
    "        df['Company Owner'] = df.apply(\n",
    "            lambda row: state_team_map.get(row['Mailing State/Province'], row['Company Owner'])\n",
    "            if pd.isna(row['Company Owner']) else row['Company Owner'],\n",
    "            axis=1\n",
    "        )\n",
    "    df['OwnerID'] = df['Company Owner'].map(team_ownerID_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e0376",
   "metadata": {},
   "source": [
    "# Dakota Phase 1: Wirehouse Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bea1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of account entries: 167025\n",
      "\n",
      "Wirehouse group counts:\n",
      "wirehouse_group\n",
      "None              88061\n",
      "Wells Fargo       19609\n",
      "Merrill Lynch     13203\n",
      "Morgan Stanley    11246\n",
      "J.P. Morgan       11034\n",
      "Edward Jones       9699\n",
      "UBS                6793\n",
      "Ameriprise         4064\n",
      "Raymond James      2805\n",
      "Goldman Sachs       274\n",
      "BlackRock           237\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the wirehouse map\n",
    "wirehouse_map = {\n",
    "    'Bank of America Merrill Lynch': 'Merrill Lynch',\n",
    "    'Wells Fargo Advisors LLC': 'Wells Fargo',\n",
    "    'Morgan Stanley &amp; Co.': 'Morgan Stanley',\n",
    "    'J.P. Morgan Securities LLC': 'J.P. Morgan',\n",
    "    'UBS Financial Services Inc.': 'UBS',\n",
    "    'Ameriprise Financial Services Inc.': 'Ameriprise',\n",
    "    'Raymond James Financial Services Inc.': 'Raymond James',\n",
    "    'Goldman Sachs &amp; Co. LLC': 'Goldman Sachs',\n",
    "    'UBS': 'UBS',\n",
    "    'Morgan Stanley': 'Morgan Stanley',\n",
    "    'Wells Fargo Advisors': 'Wells Fargo',\n",
    "    'Wells Fargo': 'Wells Fargo',\n",
    "    'Ameriprise Financial Services': 'Ameriprise',\n",
    "    'Goldman Sachs International': 'Goldman Sachs',\n",
    "    'Raymond James Financial Services Advisors, Inc': 'Raymond James',\n",
    "    'Raymond James &amp; Associates Inc.': 'Raymond James',\n",
    "    'J.P. Morgan &amp; Co.': 'J.P. Morgan',\n",
    "    'BlackRock': 'BlackRock',\n",
    "    'Edward D. Jones': 'Edward Jones',\n",
    "    'BlackRock Alternative Investors': 'BlackRock',\n",
    "    'BlackRock Alternative Investors (BAI)': 'BlackRock'\n",
    "}\n",
    "\n",
    "# Create a normalized version of the wirehouse map for matching\n",
    "normalized_map = {k.lower(): v for k, v in wirehouse_map.items()}\n",
    "\n",
    "# Fuzzy match function using normalized names\n",
    "def match_wirehouse(name):\n",
    "    normalized_name = name.strip().lower()\n",
    "    matches = difflib.get_close_matches(normalized_name, normalized_map.keys(), n=1, cutoff=0.6)\n",
    "    return normalized_map[matches[0]] if matches else None\n",
    "\n",
    "# Apply fuzzy matching without altering the original 'Account Name'\n",
    "dakota_df['wirehouse_group'] = dakota_df['Account Name'].apply(match_wirehouse)\n",
    "\n",
    "# Count and display results\n",
    "print(f\"Total number of account entries: {dakota_df['Account Name'].count()}\")\n",
    "print(\"\\nWirehouse group counts:\")\n",
    "print(dakota_df['wirehouse_group'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edcc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "Number of unique emails left: 76673\n",
      "Number of unique account names left: 27872\n",
      "Number of duplicate emails: 11387\n",
      "\n",
      "Account Totals:\n",
      "Total accounts at start: 167025\n",
      "Wirehouses removed: 78964\n",
      "Total accounts left: 88061\n"
     ]
    }
   ],
   "source": [
    "# Total accounts at start\n",
    "total_accounts_start = len(dakota_df)\n",
    "\n",
    "# Split into wirehouse and non-wirehouse groups\n",
    "dakota_wirehouses = dakota_df[dakota_df['wirehouse_group'].notna()].copy()\n",
    "dakota_non_wirehouse_df = dakota_df[dakota_df['wirehouse_group'].isna()].copy()\n",
    "\n",
    "# Count wirehouses removed and accounts left\n",
    "wirehouses_removed = len(dakota_wirehouses)\n",
    "total_accounts_left = len(dakota_non_wirehouse_df)\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the wirehouse and non-wirehouse datasets with new names\n",
    "dakota_wirehouses.to_excel(os.path.join(output_dir, \"Wirehouse_Accounts.xlsx\"), index=False)\n",
    "dakota_non_wirehouse_df.to_excel(os.path.join(output_dir, \"Non_Wirehouse_Accounts.xlsx\"), index=False)\n",
    "\n",
    "# Summary stats\n",
    "num_unique_emails = dakota_non_wirehouse_df['Email'].nunique()\n",
    "num_unique_accounts = dakota_non_wirehouse_df['Account Name'].nunique()\n",
    "num_duplicate_emails = dakota_non_wirehouse_df['Email'].duplicated().sum()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"Number of unique emails left: {num_unique_emails}\")\n",
    "print(f\"Number of unique account names left: {num_unique_accounts}\")\n",
    "print(f\"Number of duplicate emails: {num_duplicate_emails}\")\n",
    "\n",
    "print(\"\\nAccount Totals:\")\n",
    "print(f\"Total accounts at start: {total_accounts_start}\")\n",
    "print(f\"Wirehouses removed: {wirehouses_removed}\")\n",
    "print(f\"Total accounts left: {total_accounts_left}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7beb6",
   "metadata": {},
   "source": [
    "# Phase 2: Dupes and Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ad8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Total entries: 88061\n",
      "Blank emails: 10844\n",
      "Duplicate emails (non-blank): 1052\n",
      "Duplicate names: 5092\n",
      "Cleaned entries: 71073\n"
     ]
    }
   ],
   "source": [
    "# Total entries\n",
    "total_entries = len(dakota_non_wirehouse_df)\n",
    "\n",
    "# Create Full Name column\n",
    "dakota_non_wirehouse_df['Full Name'] = (\n",
    "    dakota_non_wirehouse_df['First Name'].astype(str).str.strip() + ' ' +\n",
    "    dakota_non_wirehouse_df['Last Name'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "# Step 1: Identify blank emails\n",
    "dakota_blanks = dakota_non_wirehouse_df[\n",
    "    dakota_non_wirehouse_df['Email'].isna() |\n",
    "    (dakota_non_wirehouse_df['Email'].astype(str).str.strip() == '')\n",
    "]\n",
    "\n",
    "# Remove blanks from main dataset\n",
    "remaining_df = dakota_non_wirehouse_df.drop(dakota_blanks.index)\n",
    "\n",
    "# Step 2: Identify duplicate emails (excluding blanks)\n",
    "non_blank_df = remaining_df[\n",
    "    ~remaining_df['Email'].isna() &\n",
    "    (remaining_df['Email'].astype(str).str.strip() != '')\n",
    "]\n",
    "dakota_dupes = non_blank_df[non_blank_df.duplicated(subset='Email', keep=False)]\n",
    "dakota_dupes = dakota_dupes.sort_values(by='Email')\n",
    "\n",
    "# Remove duplicate emails from main dataset\n",
    "remaining_df = remaining_df.drop(dakota_dupes.index)\n",
    "\n",
    "# Step 3: Identify duplicate names (excluding blanks and duplicate emails)\n",
    "dakota_name_dupes = remaining_df[\n",
    "    remaining_df.duplicated(subset='Full Name', keep=False)\n",
    "].sort_values(by='Full Name')\n",
    "\n",
    "# Final cleaned dataset (not in any of the above categories)\n",
    "cleaned_df = remaining_df.drop(dakota_name_dupes.index)\n",
    "\n",
    "# Counts\n",
    "num_blank = len(dakota_blanks)\n",
    "num_duplicates = len(dakota_dupes)\n",
    "num_name_duplicates = len(dakota_name_dupes)\n",
    "num_cleaned = len(cleaned_df)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"Blank emails: {num_blank}\")\n",
    "print(f\"Duplicate emails (non-blank): {num_duplicates}\")\n",
    "print(f\"Duplicate names: {num_name_duplicates}\")\n",
    "print(f\"Cleaned entries: {num_cleaned}\")\n",
    "\n",
    "# Save outputs\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dakota_blanks.to_excel(os.path.join(output_dir, \"Dakota_Blanks.xlsx\"), index=False)\n",
    "dakota_dupes.to_excel(os.path.join(output_dir, \"Dakota_Duplicates.xlsx\"), index=False)\n",
    "dakota_name_dupes.to_excel(os.path.join(output_dir, \"Dakota_DuplicateNames.xlsx\"), index=False)\n",
    "cleaned_df.to_excel(os.path.join(output_dir, \"Dakota_Cleaned.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf1141",
   "metadata": {},
   "source": [
    "# Dakota AUM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51338ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking AUM Data for dupes\n",
    "matched_accounts[\"Provided Account ID\"] = matched_accounts[\"Provided Account ID\"].str.strip().str.upper()\n",
    "matched_contacts[\"Provided Salesforce Contact Account Record ID\"] = matched_contacts[\"Provided Salesforce Contact Account Record ID\"].str.strip().str.upper()\n",
    "print(matched_contacts[\"Provided Salesforce Contact Account Record ID\"].isnull().sum())\n",
    "print(matched_accounts[\"Provided Account ID\"].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be13b4",
   "metadata": {},
   "source": [
    "### Matching AUM Data\n",
    "- Output only matched account IDs across 4 files\n",
    "\n",
    "    - matched contacts\n",
    "    - matched accounts\n",
    "    - unmatched contacts\n",
    "    - unmatched accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ac4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched accounts: 5391\n",
      "Matched contacts: 18374\n",
      "Unmatched accounts: 34435\n",
      "Unmatched contacts: 167025\n",
      "\n",
      "Matched contacts: 16927\n",
      "Match rate: 92.12%\n",
      "\n",
      "Unmatched contacts matched to accounts: 69419\n",
      "Unmatched match rate: 41.56%\n",
      "\n",
      "Total matched contacts: 16927\n",
      "\n",
      "NaN count per column:\n",
      "Provided Account ID                                  0\n",
      "Provided Account Name                                0\n",
      "Provided Firm ID                                 16092\n",
      "Provided BillingStreet                           12808\n",
      "Provided Billing City                            11098\n",
      "                                                 ...  \n",
      "Dakota Last Modified Date/Time (DATALOADER)_y        0\n",
      "Dakota Created Date/Time (DATALOADER)_y              0\n",
      "Dakota Status_y                                      0\n",
      "Matching Criteria_y                                  0\n",
      "Duplicates_y                                         0\n",
      "Length: 122, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "matched_accounts_path = \"../data/prospect/matched_accounts.xlsx\"\n",
    "matched_contacts_path = \"../data/prospect/matched_contacts.xlsx\"\n",
    "unmatched_accounts_path = \"../data/prospect/unmatched_accounts.xlsx\"\n",
    "unmatched_contacts_path = \"../data/prospect/unmatched_contacts.xlsx\"\n",
    "\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load matched datasets\n",
    "matched_accounts = pd.read_excel(matched_accounts_path, engine='openpyxl')\n",
    "matched_contacts = pd.read_excel(matched_contacts_path, engine='openpyxl')\n",
    "\n",
    "# Count rows before merge\n",
    "print(f\"Matched accounts: {len(matched_accounts)}\")\n",
    "print(f\"Matched contacts: {len(matched_contacts)}\")\n",
    "\n",
    "# Merge matched accounts and contacts\n",
    "aum_matched = pd.merge(\n",
    "    matched_accounts,\n",
    "    matched_contacts,\n",
    "    left_on=\"Provided Account ID\",\n",
    "    right_on=\"Provided Salesforce Contact Account Record ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save matched results\n",
    "aum_matched.to_excel(os.path.join(output_dir, \"AUM_matched.xlsx\"), index=False)\n",
    "\n",
    "# Load unmatched datasets\n",
    "unmatched_accounts = pd.read_excel(unmatched_accounts_path, engine='openpyxl')\n",
    "unmatched_contacts = pd.read_excel(unmatched_contacts_path, engine='openpyxl')\n",
    "\n",
    "# Count rows before merge\n",
    "print(f\"Unmatched accounts: {len(unmatched_accounts)}\")\n",
    "print(f\"Unmatched contacts: {len(unmatched_contacts)}\")\n",
    "\n",
    "# Attempt to match unmatched contacts to unmatched accounts\n",
    "aum_unmatched = pd.merge(\n",
    "    unmatched_accounts,\n",
    "    unmatched_contacts,\n",
    "    on=\"Account ID (Case Safe)\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save unmatched results\n",
    "aum_unmatched.to_excel(os.path.join(output_dir, \"AUM_unmatched.xlsx\"), index=False)\n",
    "\n",
    "# Print match statistics\n",
    "match_percentage = (len(aum_matched) / len(matched_contacts)) * 100\n",
    "print(f\"\\nMatched contacts: {len(aum_matched)}\")\n",
    "print(f\"Match rate: {match_percentage:.2f}%\")\n",
    "\n",
    "unmatch_percentage = (len(aum_unmatched) / len(unmatched_contacts)) * 100\n",
    "print(f\"\\nUnmatched contacts matched to accounts: {len(aum_unmatched)}\")\n",
    "print(f\"Unmatched match rate: {unmatch_percentage:.2f}%\")\n",
    "\n",
    "# Count total contacts\n",
    "total_contacts = len(aum_matched)\n",
    "print(f\"\\nTotal matched contacts: {total_contacts}\")\n",
    "\n",
    "# Count NaN values per column\n",
    "nan_summary = aum_matched.isna().sum()\n",
    "print(\"\\nNaN count per column:\")\n",
    "print(nan_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1166ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Provided Account ID', 'Provided Account Name', 'Provided Firm ID', 'Provided BillingStreet', 'Provided Billing City', 'Provided BillingCountry', 'Provided BillingPostalCode', 'Provided Website', 'Provided CRD', 'Dakota Account ID (Case Safe)_x', 'Dakota Account Name_x', 'Dakota Type', 'Dakota AUM', 'Dakota Metro Area', 'Dakota Parent Account', 'Dakota Website', 'Dakota Phone_x', 'Dakota CRD#', 'Dakota Billing Street', 'Dakota Billing City', 'Dakota Billing State/Province', 'Dakota Billing Zip/Postal Code', 'Dakota Billing Country', 'Dakota Description', 'Dakota Platform Description', 'Dakota Research Team Overview', 'Dakota Opportunity Description', 'Dakota Custodian(s)', 'Dakota UHNW Division', 'Dakota TAMP', 'Dakota Sub-Advised MF Family', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models', 'Dakota Client Base', 'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG', 'Dakota Preferred Investment Vehicle', 'Dakota Mutual Fund Usage', 'Dakota LP Usage', 'Dakota Separate Account Usage', 'Dakota RIC Usage', 'Dakota UMA Usage', 'Dakota ETF Usage', 'Dakota CIT Usage', 'Dakota UCITS Usage', 'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF', 'Dakota General Consultant', 'Dakota General Consultant 2', 'Dakota Hedge Fund Consultant', 'Dakota Real Estate Consultant', 'Dakota Private Equity Consultant', 'Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities', 'Dakota Micro Cap US Equities', 'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities', 'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds', 'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans', 'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles', 'Dakota Private Equity', 'Dakota Private Credit', 'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives', 'Dakota Real Assets', 'Dakota Venture Capital', 'Dakota Alternative Platform Description', 'Dakota Last Modified Date/Time', 'Dakota Created Date/Time', 'Dakota Last Modified Date/Time (DATALOADER)_x', 'Dakota Created Date/Time (DATALOADER)_x', 'Dakota Status_x', 'Matching Criteria_x', 'Duplicates_x', 'Provided Contact ID', 'Provided Salesforce Contact Account Record ID', 'Provided First Name', 'Provided Last Name', 'Provided Email', 'Provided Phone', 'Provided MailingStreet', 'Provided MailingCity', 'Provided MailingState', 'Provided MailingCountry', 'Provided MailingPostalCode', 'Dakota ContactID', 'Dakota First Name', 'Dakota Last Name', 'Dakota Email', 'Dakota Account ID (Case Safe)_y', 'Dakota Account Name_y', 'Dakota Metro Area: Metro Area Name', 'Dakota Phone_y', 'Dakota Title', 'Dakota Contact Type', 'Dakota Asset Class Coverage', 'Dakota Mailing Street', 'Dakota Mailing City', 'Dakota Mailing State/Province', 'Dakota Mailing Zip/Postal Code', 'Dakota Mailing Country', 'Dakota Biography', 'Dakota CRD #', 'Dakota Last Modified DateTime', 'Dakota Created DateTime', 'Dakota Last Modified Date/Time (DATALOADER)_y', 'Dakota Created Date/Time (DATALOADER)_y', 'Dakota Status_y', 'Matching Criteria_y', 'Duplicates_y']\n"
     ]
    }
   ],
   "source": [
    "print(aum_matched.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a49f2",
   "metadata": {},
   "source": [
    "Final Dataset for AUM Structure:\n",
    "\n",
    "Provided Account ID | Provided Account Name | Provided Firm ID | Provided BillingStreet | Provided Billing City | Provided BillingCountry | Provided BillingPostalCode | Provided Website | Provided CRD | Dakota Account ID (Case Safe)_x | Dakota Account Name_x\t| Dakota Type | Dakota AUM | Dakota Metro Area | Dakota Parent Account | Dakota Website\tDakota | Phone_x | Dakota CRD# | Dakota Billing Street | Dakota Billing City | Dakota Billing State/Province | Dakota Billing Zip/Postal Code | Dakota Billing Country | Dakota Description | Dakota Platform Description | Dakota Research Team Overview | Dakota Opportunity Description | Dakota Custodian(s) | Dakota UHNW Division | Dakota TAMP | Dakota Sub-Advised MF Family | Dakota Select Lists | Dakota OCIO Business | Dakota Models | Dakota Client Base | Dakota Emerging Manager Program | Dakota Invests in Impact, SRI or ESG | Dakota Preferred Investment Vehicle | Dakota Mutual Fund Usage | Dakota LP Usage | Dakota Separate Account Usage | Dakota RIC Usage | Dakota UMA Usage | Dakota ETF Usage | Dakota CIT Usage | Dakota UCITS Usage | Dakota Hedge FOF | Dakota Real Estate FOF | Dakota Private Equity FOF\tDakota General Consultant | Dakota General Consultant 2\t| Dakota Hedge Fund Consultant | Dakota Real Estate Consultant | Dakota Private Equity Consultant | Dakota Small Cap Equities | Dakota Mid Cap Equities\tDakota Large Cap Equities | Dakota Micro Cap US Equities | Dakota International Equities |\tDakota Emerging Market Equities | Dakota Global Equities | Dakota Municipal Bonds | Dakota Core Bonds | Dakota Emerging Market Bonds | Dakota High Yield Bonds | Dakota Government Bonds | Dakota Bank Loans | Dakota Unconstrained\tDakota MBS | Dakota CMBS | Dakota Convertibles | Dakota Private Equity | Dakota Private Credit | Dakota Hedge Funds | Dakota Private Real Estate | Dakota Liquid Alternatives | Dakota Real Assets | Dakota Venture Capital | Dakota Alternative Platform Description | Dakota Last Modified Date/Time | Dakota Created Date/Time | Dakota Last Modified Date/Time (DATALOADER)_x | Dakota Created Date/Time (DATALOADER)_x | Dakota Status_x | Matching Criteria_x | Duplicates_x | Provided Contact ID | Provided Salesforce Contact Account Record ID | Provided First Name | Provided Last Name | Provided Email | Provided Phone | Provided MailingStreet | Provided MailingCity | Provided MailingState | Provided MailingCountry | Provided MailingPostalCode | Dakota ContactID |\tDakota First Name | Dakota Last Name | Dakota Email | Dakota Account ID (Case Safe)_y | Dakota Account Name_y | Dakota Metro Area: Metro Area Name | Dakota Phone_y | Dakota Title | Dakota Contact Type | Dakota Asset Class Coverage | Dakota Mailing Street | Dakota Mailing City | Dakota Mailing State/Province | Dakota Mailing Zip/Postal Code | Dakota Mailing Country | Dakota Biography | Dakota CRD # | Dakota Last Modified DateTime | Dakota Created DateTime | Dakota Last Modified Date/Time (DATALOADER)_y | Dakota Created Date/Time (DATALOADER)_y | Dakota Status_y | Matching Criteria_y | Duplicates_y | Converted?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053da8e",
   "metadata": {},
   "source": [
    "# Matching win list to labeled dataset\n",
    "- Of the historical wins, what % can be mapped to labels?\n",
    "- 'Provided Account Name' - 'Potential Client'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f421b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:57: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[vehicle_col] = aum_labeled[vehicle_col].replace(vehicle_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_49472\\115413675.py:72: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aum_labeled[col] = aum_labeled[col].replace(usage_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Yes/No columns to binary: ['Provided CRD', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models', 'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG', 'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF', 'Dakota Micro Cap US Equities', 'Dakota Private Equity', 'Dakota Private Credit', 'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives', 'Dakota Real Assets', 'Dakota Venture Capital']\n",
      "Converted strategy columns: ['Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities', 'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities', 'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds', 'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans', 'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles', 'Dakota Status_x', 'Dakota Status_y']\n",
      "Unmapped values found in strategy columns:\n",
      "Dakota Small Cap Equities: ['0.0', '1.0', '3.0', '2.0']\n",
      "Dakota Mid Cap Equities: ['0.0', '1.0', '2.0', '3.0']\n",
      "Dakota Large Cap Equities: ['0.0', '2.0', '3.0', '1.0']\n",
      "Dakota International Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Emerging Market Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Global Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Municipal Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Core Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Emerging Market Bonds: ['0.0', '1.0', '3.0']\n",
      "Dakota High Yield Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Government Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Bank Loans: ['1.0', '0.0', '3.0']\n",
      "Dakota Unconstrained: ['1.0', '0.0', '3.0']\n",
      "Dakota MBS: ['1.0', '0.0', '3.0']\n",
      "Dakota CMBS: ['1.0', '0.0', '3.0']\n",
      "Dakota Convertibles: ['1.0', '0.0', '3.0']\n",
      "Dakota Status_x: ['1']\n",
      "Dakota Status_y: ['1']\n",
      "Unmapped values in 'Dakota Preferred Investment Vehicle': []\n",
      "Unmapped values in usage columns:\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled AUM dataset\n",
    "aum_labeled = pd.read_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Convert Yes/No columns to binary 1/0\n",
    "yes_no_columns = [\n",
    "    'Provided CRD', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models',\n",
    "    'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG',\n",
    "    'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF',\n",
    "    'Dakota Micro Cap US Equities', 'Dakota Private Equity', 'Dakota Private Credit',\n",
    "    'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives',\n",
    "    'Dakota Real Assets', 'Dakota Venture Capital'\n",
    "]\n",
    "aum_labeled[yes_no_columns] = aum_labeled[yes_no_columns].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Define strategy mapping\n",
    "strategy_map = {\n",
    "    'Active': 1,\n",
    "    'Passive': 2,\n",
    "    'Active/Passive': 0,\n",
    "    'In-House': 3,\n",
    "    'Yes': 1,\n",
    "    'No': 0\n",
    "}\n",
    "\n",
    "# Strategy columns to convert\n",
    "strategy_columns = [\n",
    "    'Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities',\n",
    "    'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities',\n",
    "    'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds',\n",
    "    'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans',\n",
    "    'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles',\n",
    "    'Dakota Status_x', 'Dakota Status_y'\n",
    "]\n",
    "\n",
    "# Mapping for Dakota Preferred Investment Vehicle\n",
    "vehicle_map = {\n",
    "    'Mutual Funds': 1,\n",
    "    'ETFs': 2,\n",
    "    'Separate Accounts': 3,\n",
    "    'LP': 4,\n",
    "    'Hedge Funds': 5,\n",
    "    'Subadvisory': 6\n",
    "}\n",
    "\n",
    "# Mapping for usage levels\n",
    "usage_map = {\n",
    "    'Zero': 0,\n",
    "    'Small': 1,\n",
    "    'Medium': 2,\n",
    "    'Large': 3\n",
    "}\n",
    "\n",
    "# Apply vehicle mapping\n",
    "vehicle_col = 'Dakota Preferred Investment Vehicle'\n",
    "unmapped_vehicles = aum_labeled[vehicle_col].dropna().astype(str).str.strip().unique()\n",
    "unmapped_vehicles = [val for val in unmapped_vehicles if val not in vehicle_map]\n",
    "aum_labeled[vehicle_col] = aum_labeled[vehicle_col].replace(vehicle_map)\n",
    "\n",
    "# Apply usage mapping\n",
    "usage_columns = [\n",
    "    'Dakota Mutual Fund Usage', 'Dakota LP Usage', 'Dakota Separate Account Usage',\n",
    "    'Dakota RIC Usage', 'Dakota UMA Usage', 'Dakota ETF Usage',\n",
    "    'Dakota CIT Usage', 'Dakota UCITS Usage'\n",
    "]\n",
    "\n",
    "unmapped_usage = {}\n",
    "for col in usage_columns:\n",
    "    unique_vals = aum_labeled[col].dropna().astype(str).str.strip().unique()\n",
    "    unmapped = [val for val in unique_vals if val not in usage_map]\n",
    "    if unmapped:\n",
    "        unmapped_usage[col] = unmapped\n",
    "    aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
    "\n",
    "# Save the updated dataset\n",
    "aum_labeled.to_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", index=False)\n",
    "\n",
    "# Track unmapped values\n",
    "unmapped_values = {}\n",
    "\n",
    "# Apply strategy mapping and collect unmapped values\n",
    "for col in strategy_columns:\n",
    "    unique_vals = aum_labeled[col].dropna().astype(str).str.strip().unique()\n",
    "    unmapped = [val for val in unique_vals if val not in strategy_map]\n",
    "    if unmapped:\n",
    "        unmapped_values[col] = unmapped\n",
    "    aum_labeled[col] = aum_labeled[col].replace(strategy_map)\n",
    "\n",
    "# Save the updated dataset\n",
    "aum_labeled.to_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", index=False)\n",
    "\n",
    "# Output summary\n",
    "print(\"Converted Yes/No columns to binary:\", yes_no_columns)\n",
    "print(\"Converted strategy columns:\", strategy_columns)\n",
    "print(\"Unmapped values found in strategy columns:\")\n",
    "for col, vals in unmapped_values.items():\n",
    "    print(f\"{col}: {vals}\")\n",
    "\n",
    "print(\"Unmapped values in 'Dakota Preferred Investment Vehicle':\", unmapped_vehicles)\n",
    "print(\"Unmapped values in usage columns:\")\n",
    "for col, vals in unmapped_usage.items():\n",
    "    print(f\"{col}: {vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5a83",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebb556",
   "metadata": {},
   "source": [
    "# Salesforce Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "322a86bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce Summary:\n",
      "Total entries: 48678\n",
      "Blank emails: 9864\n",
      "Duplicate emails (non-blank): 17\n",
      "Duplicate names: 2018\n",
      "Cleaned entries: 36779\n"
     ]
    }
   ],
   "source": [
    "# Reload the full Salesforce dataset\n",
    "salesforce_df = pd.read_excel(\"../data/all_contacts/Salesforce_dataset.xlsx\", engine=\"openpyxl\", dtype=str)\n",
    "\n",
    "# Create Full Name column\n",
    "salesforce_df['Full Name'] = (\n",
    "    salesforce_df['First Name'].astype(str).str.strip() + ' ' +\n",
    "    salesforce_df['Last Name'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "# Total entries (including blanks)\n",
    "total_entries = len(salesforce_df)\n",
    "\n",
    "# Step 1: Identify blank emails\n",
    "sf_blanks = salesforce_df[\n",
    "    salesforce_df['Email'].isna() |\n",
    "    (salesforce_df['Email'].astype(str).str.strip() == '')\n",
    "]\n",
    "\n",
    "# Remove blanks from main dataset\n",
    "remaining_df = salesforce_df.drop(sf_blanks.index)\n",
    "\n",
    "# Step 2: Identify duplicate emails (excluding blanks)\n",
    "non_blank_df = remaining_df[\n",
    "    ~remaining_df['Email'].isna() &\n",
    "    (remaining_df['Email'].astype(str).str.strip() != '')\n",
    "]\n",
    "sf_dupes = non_blank_df[non_blank_df.duplicated(subset='Email', keep=False)]\n",
    "sf_dupes = sf_dupes.sort_values(by='Email')\n",
    "\n",
    "# Remove duplicate emails from main dataset\n",
    "remaining_df = remaining_df.drop(sf_dupes.index)\n",
    "\n",
    "# Step 3: Identify duplicate names (excluding blanks and duplicate emails)\n",
    "sf_name_dupes = remaining_df[\n",
    "    remaining_df.duplicated(subset='Full Name', keep=False)\n",
    "].sort_values(by='Full Name')\n",
    "\n",
    "# Final cleaned dataset (not in any of the above categories)\n",
    "sf_cleaned = remaining_df.drop(sf_name_dupes.index)\n",
    "\n",
    "# Counts\n",
    "num_blank = len(sf_blanks)\n",
    "num_duplicates = len(sf_dupes)\n",
    "num_name_duplicates = len(sf_name_dupes)\n",
    "num_cleaned = len(sf_cleaned)\n",
    "\n",
    "# Print summary\n",
    "print(\"Salesforce Summary:\")\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"Blank emails: {num_blank}\")\n",
    "print(f\"Duplicate emails (non-blank): {num_duplicates}\")\n",
    "print(f\"Duplicate names: {num_name_duplicates}\")\n",
    "print(f\"Cleaned entries: {num_cleaned}\")\n",
    "\n",
    "# Save outputs to the same Dakota folder with Salesforce-specific names\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sf_blanks.to_excel(os.path.join(output_dir, \"Salesforce_Blanks.xlsx\"), index=False)\n",
    "sf_dupes.to_excel(os.path.join(output_dir, \"Salesforce_Duplicates.xlsx\"), index=False)\n",
    "sf_name_dupes.to_excel(os.path.join(output_dir, \"Salesforce_DuplicateNames.xlsx\"), index=False)\n",
    "sf_cleaned.to_excel(os.path.join(output_dir, \"Salesforce_Cleaned.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56195c",
   "metadata": {},
   "source": [
    "# Non Wirehouse Dakota X Salesforce Match\n",
    "- Compare and contrast\n",
    "- Match with Dakota overriding uncertain salesforce bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6bd45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overlap and Merge Summary ===\n",
      "Total Dakota entries: 71073\n",
      "Total Salesforce entries: 36779\n",
      "Overlap by Email: 298\n",
      "Overlap by Name only: 2698\n",
      "Dakota contacts to merge: 68079\n",
      "Salesforce contacts to merge: 33790\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned datasets\n",
    "dakota_df = pd.read_excel(\"../data/dakota_salesforce_sheets/Dakota_Cleaned.xlsx\", engine=\"openpyxl\")\n",
    "salesforce_df = pd.read_excel(\"../data/dakota_salesforce_sheets/Salesforce_Cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Normalize email and full name for matching\n",
    "dakota_df['Email_norm'] = dakota_df['Email'].astype(str).str.strip().str.lower()\n",
    "salesforce_df['Email_norm'] = salesforce_df['Email'].astype(str).str.strip().str.lower()\n",
    "\n",
    "dakota_df['Full Name_norm'] = dakota_df['First Name'].astype(str).str.strip().str.lower() + ' ' + dakota_df['Last Name'].astype(str).str.strip().str.lower()\n",
    "salesforce_df['Full Name_norm'] = salesforce_df['First Name'].astype(str).str.strip().str.lower() + ' ' + salesforce_df['Last Name'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Identify overlaps by email\n",
    "email_overlap = pd.merge(\n",
    "    dakota_df, salesforce_df,\n",
    "    on='Email_norm',\n",
    "    suffixes=('_dakota', '_salesforce')\n",
    ")\n",
    "\n",
    "# Identify overlaps by full name (excluding those already matched by email)\n",
    "email_overlap_keys = set(email_overlap['Email_norm'])\n",
    "name_overlap = pd.merge(\n",
    "    dakota_df, salesforce_df,\n",
    "    on='Full Name_norm',\n",
    "    suffixes=('_dakota', '_salesforce')\n",
    ")\n",
    "name_overlap = name_overlap[~name_overlap['Email_norm_dakota'].isin(email_overlap_keys)]\n",
    "\n",
    "# Identify non-overlapping Dakota entries\n",
    "dakota_non_overlap = dakota_df[\n",
    "    ~dakota_df['Email_norm'].isin(email_overlap['Email_norm']) &\n",
    "    ~dakota_df['Full Name_norm'].isin(name_overlap['Full Name_norm'])\n",
    "]\n",
    "\n",
    "# Identify non-overlapping Salesforce entries\n",
    "salesforce_non_overlap = salesforce_df[\n",
    "    ~salesforce_df['Email_norm'].isin(email_overlap['Email_norm']) &\n",
    "    ~salesforce_df['Full Name_norm'].isin(name_overlap['Full Name_norm'])\n",
    "]\n",
    "\n",
    "# Print counts\n",
    "print(\"=== Overlap and Merge Summary ===\")\n",
    "print(f\"Total Dakota entries: {len(dakota_df)}\")\n",
    "print(f\"Total Salesforce entries: {len(salesforce_df)}\")\n",
    "print(f\"Overlap by Email: {len(email_overlap)}\")\n",
    "print(f\"Overlap by Name only: {len(name_overlap)}\")\n",
    "print(f\"Dakota contacts to merge: {len(dakota_non_overlap)}\")\n",
    "print(f\"Salesforce contacts to merge: {len(salesforce_non_overlap)}\")\n",
    "\n",
    "# Save outputs\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "email_overlap.to_excel(os.path.join(output_dir, \"Overlap_By_Email.xlsx\"), index=False)\n",
    "name_overlap.to_excel(os.path.join(output_dir, \"Overlap_By_Name.xlsx\"), index=False)\n",
    "dakota_non_overlap.to_excel(os.path.join(output_dir, \"Dakota_Contacts_Merge_List.xlsx\"), index=False)\n",
    "salesforce_non_overlap.to_excel(os.path.join(output_dir, \"Salesforce_Contacts_Merge_List.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc3089",
   "metadata": {},
   "source": [
    "# Field Matching\n",
    "Dakota\n",
    " 0   ContactID                    76461 non-null  object - not in salesforce        \n",
    " 1   First Name                   76431 non-null  object - First Name\n",
    " 2   Last Name                    76461 non-null  object - Last Name\n",
    " 3   Email                        76460 non-null  object - Email\n",
    " 4   Account ID (Case Safe)       76461 non-null  object - not in salesforce\n",
    " 5   Account Name                 76461 non-null  object - Company Name\n",
    " 6   Metro Area: Metro Area Name  76439 non-null  object - not in salesforce but derivable from city\n",
    " 7   Phone                        75640 non-null  object - Phone\n",
    " 8   Title                        76457 non-null  object - Title\n",
    " 9   Contact Type                 76453 non-null  object - very similar to title\n",
    " 10  Asset Class Coverage         76438 non-null  object - unique Dakota\n",
    " 11  Mailing Street               75938 non-null  object - same \n",
    " 12  Mailing City                 76441 non-null  object - same \n",
    " 13  Mailing State/Province       62511 non-null  object - same \n",
    " 14  Mailing Zip/Postal Code      75830 non-null  object - same \n",
    " 15  Mailing Country              76456 non-null  object - same \n",
    " 16  Biography                    28665 non-null  object - text about their background\n",
    " 17  CRD #                        30611 non-null  object - not in salesforce\n",
    " 18  Last Modified DateTime       76461 non-null  object - not needed\n",
    " 19  Created DateTime             76461 non-null  object - not needed\n",
    " 20  Unnamed: 20                  1 non-null      object - N/A\n",
    " 21  wirehouse_group              0 non-null      object - N/A\n",
    "\n",
    "Salesforce\n",
    "---  ------                   --------------  ----- \n",
    " 0   Salutation               3 non-null      object - Not in Dakota and many empty \n",
    " 1   First Name               37523 non-null  object - same\n",
    " 2   Last Name                37629 non-null  object - same\n",
    " 3   Title                    35354 non-null  object - same\n",
    " 4   Company Name             37629 non-null  object - Account Name\n",
    " 5   Mailing Street           27010 non-null  object - same \n",
    " 6   Mailing City             36202 non-null  object - same \n",
    " 7   Mailing State/Province   35350 non-null  object - same \n",
    " 8   Mailing Zip/Postal Code  15551 non-null  object - same \n",
    " 9   Mailing Country          36343 non-null  object - same \n",
    " 10  Phone                    31198 non-null  object - same\n",
    " 11  Fax                      10095 non-null  object - not in Dakota\n",
    " 12  Mobile                   5 non-null      object - not in Dakota\n",
    " 13  Email                    37628 non-null  object - same\n",
    " 14  Company Owner            37629 non-null  object - if not existing, make it Rockefeller Asset Management? \n",
    " 15  wirehouse_group          0 non-null      object - N/A \n",
    "\n",
    " New DB\n",
    "---  ------                   --------------  ----- \n",
    "0 First Name\n",
    "1 Last Name\n",
    "2 Title\n",
    "3 Company Name (merge with account name)\n",
    "4 Email\n",
    "5 Mailing Street\n",
    "6 Mailing City\n",
    "7 Mailing State/Province\n",
    "8 Mailing Zip/Postal Code\n",
    "9 Mailing Country \n",
    "10 Phone\n",
    "11 Company Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8577fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Missing values in Salesforce before merge:\n",
      "Salutation                 37625\n",
      "First Name                   106\n",
      "Last Name                      0\n",
      "Title                       2275\n",
      "Company Name                   0\n",
      "Mailing Street             10619\n",
      "Mailing City                1427\n",
      "Mailing State/Province      2279\n",
      "Mailing Zip/Postal Code    22078\n",
      "Mailing Country                0\n",
      "Phone                       6431\n",
      "Fax                        27533\n",
      "Mobile                     37623\n",
      "Email                          0\n",
      "Company Owner                  0\n",
      "wirehouse_group            37628\n",
      "dtype: int64\n",
      "🔍 Missing values in Dakota before merge:\n",
      "ContactID                          0\n",
      "First Name                        30\n",
      "Last Name                          0\n",
      "Email                              0\n",
      "Account ID (Case Safe)             0\n",
      "Company Name                       0\n",
      "Metro Area: Metro Area Name       22\n",
      "Phone                            821\n",
      "Title                              4\n",
      "Contact Type                       8\n",
      "Asset Class Coverage              23\n",
      "Mailing Street                   523\n",
      "Mailing City                      20\n",
      "Mailing State/Province         13949\n",
      "Mailing Zip/Postal Code          631\n",
      "Mailing Country                    0\n",
      "Biography                      47796\n",
      "CRD #                          45849\n",
      "Last Modified DateTime             0\n",
      "Created DateTime                   0\n",
      "Unnamed: 20                    76459\n",
      "wirehouse_group                76460\n",
      "dtype: int64\n",
      "📌 Salesforce duplicate emails before dropping: 0\n",
      "📌 Dakota duplicate emails before dropping: 0\n",
      "🔍 Missing values in merged dataset:\n",
      "Salutation                     113747\n",
      "First Name_sf                   76228\n",
      "Last Name_sf                    76122\n",
      "Title_sf                        78397\n",
      "Company Name_sf                 76122\n",
      "Mailing Street_sf               86741\n",
      "Mailing City_sf                 77549\n",
      "Mailing State/Province_sf       78401\n",
      "Mailing Zip/Postal Code_sf      98200\n",
      "Mailing Country_sf              76122\n",
      "Phone_sf                        82553\n",
      "Fax                            103655\n",
      "Mobile                         113745\n",
      "Email                               0\n",
      "Company Owner                   76122\n",
      "wirehouse_group_sf             113750\n",
      "ContactID                       37290\n",
      "First Name_dk                   37320\n",
      "Last Name_dk                    37290\n",
      "Account ID (Case Safe)          37290\n",
      "Company Name_dk                 37290\n",
      "Metro Area: Metro Area Name     37312\n",
      "Phone_dk                        38111\n",
      "Title_dk                        37294\n",
      "Contact Type                    37298\n",
      "Asset Class Coverage            37313\n",
      "Mailing Street_dk               37813\n",
      "Mailing City_dk                 37310\n",
      "Mailing State/Province_dk       51239\n",
      "Mailing Zip/Postal Code_dk      37921\n",
      "Mailing Country_dk              37290\n",
      "Biography                       85086\n",
      "CRD #                           83139\n",
      "Last Modified DateTime          37290\n",
      "Created DateTime                37290\n",
      "Unnamed: 20                    113749\n",
      "wirehouse_group_dk             113750\n",
      "dtype: int64\n",
      "📊 Total contacts after merge: 113750\n",
      "🆕 New contacts acquired from Dakota: 76122\n",
      "✅ Dakota contacts already covered: 0.44%\n",
      "          First Name Last Name                        Title  \\\n",
      "0  Chris (Soon Shin)       Cho  Head of External Management   \n",
      "1            Grayham    Lohrey                          NaN   \n",
      "2              Kathy    Kaiser               Plan Executive   \n",
      "3              Emily     Fluke                          NaN   \n",
      "4               Zach      Noke                          NaN   \n",
      "\n",
      "                           Company Name  \\\n",
      "0          Korea Investment Corporation   \n",
      "1                 Nagle Wealth Partners   \n",
      "2  Electrical Workers, Ibew, Local #369   \n",
      "3                 Nagle Wealth Partners   \n",
      "4                 Nagle Wealth Partners   \n",
      "\n",
      "                                               Email  \\\n",
      "0                              #kic_eq_efm@kic.go.kr   \n",
      "1  01beb9b49b8d4c8e896880a027bf3b3eglohrey@rockco...   \n",
      "2                      369memberhelp@369benefits.com   \n",
      "3  583d237fbf0944ab98137ff29edb55fbefluke@rockco.com   \n",
      "4   73568579004c452b83ea163d7ec57082znoke@rockco.com   \n",
      "\n",
      "                Mailing Street Mailing City Mailing State/Province  \\\n",
      "0                          NaN        Seoul                    NaN   \n",
      "1                          NaN          NaN                    NaN   \n",
      "2  4315 Preston Hwy. Suite 102   Louisville                     KY   \n",
      "3                          NaN          NaN                    NaN   \n",
      "4                          NaN          NaN                    NaN   \n",
      "\n",
      "  Mailing Zip/Postal Code Mailing Country           Phone  Company Owner  \n",
      "0                     NaN     South Korea    212-644-0925            NaN  \n",
      "1                     NaN                             NaN            NaN  \n",
      "2                   40217   United States  (502) 635-2611            NaN  \n",
      "3                     NaN                             NaN            NaN  \n",
      "4                     NaN                             NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Standardize emails\n",
    "salesforce_non_wirehouse_df['Email'] = salesforce_non_wirehouse_df['Email'].str.strip().str.lower()\n",
    "dakota_non_wirehouse_df['Email'] = dakota_non_wirehouse_df['Email'].str.strip().str.lower()\n",
    "\n",
    "# Rename 'Account Name' in Dakota to 'Company Name' to standardize BEFORE merging\n",
    "if 'Account Name' in dakota_non_wirehouse_df.columns:\n",
    "    dakota_non_wirehouse_df = dakota_non_wirehouse_df.rename(columns={'Account Name': 'Company Name'})\n",
    "\n",
    "# Drop rows with missing emails\n",
    "salesforce_non_wirehouse_df = salesforce_non_wirehouse_df.dropna(subset=['Email'])\n",
    "dakota_non_wirehouse_df = dakota_non_wirehouse_df.dropna(subset=['Email'])\n",
    "\n",
    "# Check how many NaNs existed before the merge\n",
    "print(\"🔍 Missing values in Salesforce before merge:\")\n",
    "print(salesforce_non_wirehouse_df.isna().sum())\n",
    "\n",
    "print(\"🔍 Missing values in Dakota before merge:\")\n",
    "print(dakota_non_wirehouse_df.isna().sum())\n",
    "\n",
    "# Count and show duplicates before dropping\n",
    "print(\"📌 Salesforce duplicate emails before dropping:\",\n",
    "      salesforce_non_wirehouse_df.duplicated(subset='Email').sum())\n",
    "print(\"📌 Dakota duplicate emails before dropping:\",\n",
    "      dakota_non_wirehouse_df.duplicated(subset='Email').sum())\n",
    "\n",
    "# Drop duplicate emails\n",
    "salesforce_non_wirehouse_df = salesforce_non_wirehouse_df.drop_duplicates(subset='Email')\n",
    "dakota_non_wirehouse_df = dakota_non_wirehouse_df.drop_duplicates(subset='Email')\n",
    "\n",
    "# Merge datasets on Email\n",
    "merged = pd.merge(\n",
    "    salesforce_non_wirehouse_df,\n",
    "    dakota_non_wirehouse_df,\n",
    "    on='Email',\n",
    "    how='outer',\n",
    "    suffixes=('_sf', '_dk')\n",
    ")\n",
    "\n",
    "# Check how many NaNs exist after the merge\n",
    "print(\"🔍 Missing values in merged dataset:\")\n",
    "print(merged.isna().sum())\n",
    "\n",
    "# Override Salesforce fields with Dakota where available\n",
    "for col in ['First Name', 'Last Name', 'Title', 'Mailing Street', 'Mailing City',\n",
    "            'Mailing State/Province', 'Mailing Zip/Postal Code', 'Mailing Country', 'Phone']:\n",
    "    col_sf = f\"{col}_sf\"\n",
    "    col_dk = f\"{col}_dk\"\n",
    "    merged[col] = merged.get(col_dk, pd.Series(index=merged.index)).combine_first(\n",
    "                  merged.get(col_sf, pd.Series(index=merged.index)))\n",
    "\n",
    "merged['Company Name'] = merged.get('Company Name_dk', pd.Series(index=merged.index)).combine_first(\n",
    "                         merged.get('Company Name_sf', pd.Series(index=merged.index)))\n",
    "\n",
    "# Capitalize company names\n",
    "merged['Company Name'] = merged['Company Name'].str.title()\n",
    "\n",
    "\n",
    "# Use Salesforce's Company Owner if available\n",
    "merged['Company Owner'] = merged.get('Company Owner_sf', pd.Series(index=merged.index))\n",
    "\n",
    "# Analytics\n",
    "total_contacts = len(merged)\n",
    "new_contacts = merged['Email'].isin(dakota_non_wirehouse_df['Email']) & ~merged['Email'].isin(salesforce_non_wirehouse_df['Email'])\n",
    "num_new_contacts = new_contacts.sum()\n",
    "coverage_pct = 100 * (1 - num_new_contacts / len(dakota_non_wirehouse_df))\n",
    "\n",
    "print(f\"📊 Total contacts after merge: {total_contacts}\")\n",
    "print(f\"🆕 New contacts acquired from Dakota: {num_new_contacts}\")\n",
    "print(f\"✅ Dakota contacts already covered: {coverage_pct:.2f}%\")\n",
    "\n",
    "# Final DataFrame\n",
    "final_columns = ['First Name', 'Last Name', 'Title', 'Company Name', 'Email',\n",
    "                 'Mailing Street', 'Mailing City', 'Mailing State/Province',\n",
    "                 'Mailing Zip/Postal Code', 'Mailing Country', 'Phone', 'Company Owner']\n",
    "final_df = merged[final_columns]\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
