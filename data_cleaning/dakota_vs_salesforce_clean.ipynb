{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba9512",
   "metadata": {},
   "source": [
    "# Dakota vs Salesforce Data Clean\n",
    "- USA name standardization done for both in load block\n",
    "- Map state ownership for both\n",
    "\n",
    "## Dakota Clean Up\n",
    "Raw | Wirehouses | Blank Emails, Dupe Emails, Dupe Names | Clean | AUM Aggregated\n",
    "\n",
    "# Salesforce Clean Up\n",
    "Raw | Dupe Emails | Dupe Names | Empty Emails | Clean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fae72d",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60deb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (2.3.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (3.1.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\markbogorad\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl matplotlib \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import difflib\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1568a",
   "metadata": {},
   "source": [
    "# Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cfd2edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x000001EC5ADD0A40>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\zipfile\\__init__.py\", line 1953, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\zipfile\\__init__.py\", line 1970, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    }
   ],
   "source": [
    "dakota_df=pd.read_excel(\"../data/all_contacts/Dakota_All_Contacts.xlsx\")\n",
    "matched_contacts=pd.read_excel(\"../data/prospect/matched_contacts.xlsx\")\n",
    "matched_accounts=pd.read_excel(\"../data/prospect/matched_accounts.xlsx\")\n",
    "unmatched_contacts=pd.read_excel(\"../data/prospect/unmatched_contacts.xlsx\")\n",
    "unmatched_accounts=pd.read_excel(\"../data/prospect/unmatched_accounts.xlsx\")\n",
    "\n",
    "salesforce_df = pd.read_excel(\"../data/all_contacts/Salesforce_dataset.xlsx\") # Josh approved\n",
    "salesforce_618 = pd.read_excel(\"../data/all_contacts/Contact_SF_Extract_6_16_25.xlsx\") # find out difference between this and the above\n",
    "\n",
    "salesforce_accts = pd.read_excel(\"../data/dakota_salesforce_sheets/SF_accounts.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f24dfd",
   "metadata": {},
   "source": [
    "# Mutual Changes\n",
    "- U.S Standardization\n",
    "- Ownership Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb65bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standardization function\n",
    "def standardize_country(country, state):\n",
    "    if pd.isnull(country):\n",
    "        country = ''\n",
    "    country_upper = str(country).strip().upper()\n",
    "    if country_upper in ['UNITED STATES', 'U.S.', 'USA', 'US']:\n",
    "        return 'United States'\n",
    "    if pd.notnull(state) and str(state).strip() != '':\n",
    "        return 'United States'\n",
    "    return country\n",
    "\n",
    "# List of possible column names for country and state\n",
    "country_column_candidates = [\n",
    "    'Mailing Country', 'MailingCountry', 'Billing Country', 'BillingCountry',\n",
    "    'Provided BillingCountry', 'Dakota Billing Country', 'Provided MailingCountry', 'Dakota Mailing Country'\n",
    "]\n",
    "state_column_candidates = [\n",
    "    'Mailing State/Province', 'Mailing State', 'Billing State/Province', 'Billing State'\n",
    "]\n",
    "\n",
    "# Function to find the first matching column from a list of candidates\n",
    "def find_column(df, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# Apply standardization to a list of DataFrames\n",
    "def apply_country_standardization(dfs):\n",
    "    for df in dfs:\n",
    "        country_col = find_column(df, country_column_candidates)\n",
    "        state_col = find_column(df, state_column_candidates)\n",
    "        if country_col:\n",
    "            df['Mailing Country'] = df.apply(\n",
    "                lambda row: standardize_country(row.get(country_col), row.get(state_col)),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "# Example usage:\n",
    "apply_country_standardization([dakota_df, matched_contacts, matched_accounts, unmatched_contacts, unmatched_accounts, salesforce_df, salesforce_618])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07eb988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state → team member (Institutional and Wealth)\n",
    "state_team_map = {\n",
    "    'AL': 'AG', 'AK': 'Rockefeller Asset Management', 'AZ': 'JC', 'AR': 'Rockefeller Asset Management', 'CA': 'JC', 'CO': 'JC',\n",
    "    'CT': 'JH', 'DE': 'KH', 'FL': 'JH', 'GA': 'AP', 'HI': 'JC', 'ID': 'JC',\n",
    "    'IL': 'MJ', 'IN': 'KH', 'IA': 'MJ', 'KS': 'Rockefeller Asset Management', 'KY': 'AG', 'LA': 'Rockefeller Asset Management',\n",
    "    'ME': 'JH', 'MD': 'KH', 'MA': 'JH', 'MI': 'KH', 'MN': 'MJ', 'MS': 'Rockefeller Asset Management',\n",
    "    'MO': 'AG', 'MT': 'JC', 'NE': 'Rockefeller Asset Management', 'NV': 'JC', 'NH': 'JH', 'NJ': 'KH',\n",
    "    'NM': 'JC', 'NY': 'AG', 'NC': 'AG', 'ND': 'Rockefeller Asset Management',\n",
    "    'OH': 'KH', 'OK': 'MJ', 'OR': 'JC', 'PA': 'KH',\n",
    "    'RI': 'JH', 'SC': 'AG', 'SD': 'Rockefeller Asset Management', 'TN': 'AG', 'TX': 'MJ', 'UT': 'JC',\n",
    "    'VT': 'JH', 'VA': 'KH', 'WA': 'JC', 'WV': 'KH', 'WI': 'MJ', 'WY': 'JC', 'DC': 'KH'\n",
    "} # Put NY as AG for ease of use\n",
    "\n",
    "# Define team member → OwnerID (leave blank if unknown)\n",
    "team_ownerID_map = {\n",
    "    'AG': '005a600000177CHAAY',\n",
    "    'MJ': '005a60000043nujAAA',\n",
    "    'KH': '005a6000000c1j8AAA',\n",
    "    'AP': '005a6000000c1jDAAQ',\n",
    "    'Rockefeller Asset Management': '005a6000005CXtNAAW',\n",
    "    'JC': '005a6000007bFIcAAM',\n",
    "    'JH': '005a6000007TD5tAAG',\n",
    "}\n",
    "\n",
    "# Apply mapping to a list of dataframes\n",
    "dataframes = {\n",
    "    \"dakota_df\": dakota_df,\n",
    "    \"matched_contacts\": matched_contacts,\n",
    "    \"matched_accounts\": matched_accounts,\n",
    "    \"unmatched_contacts\": unmatched_contacts,\n",
    "    \"unmatched_accounts\": unmatched_accounts,\n",
    "    \"salesforce_df\": salesforce_df,\n",
    "    \"salesforce_618\": salesforce_618,\n",
    "    \"salesforce_accts\": salesforce_accts\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    if 'Company Owner' not in df.columns:\n",
    "        df['Company Owner'] = None\n",
    "    if 'Mailing State/Province' in df.columns:\n",
    "        df['Company Owner'] = df.apply(\n",
    "            lambda row: state_team_map.get(row['Mailing State/Province'], row['Company Owner'])\n",
    "            if pd.isna(row['Company Owner']) else row['Company Owner'],\n",
    "            axis=1\n",
    "        )\n",
    "    df['OwnerID'] = df['Company Owner'].map(team_ownerID_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e0376",
   "metadata": {},
   "source": [
    "# Dakota Phase 1: Wirehouse Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bea1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of account entries: 167025\n",
      "\n",
      "Wirehouse group counts:\n",
      "wirehouse_group\n",
      "None              88061\n",
      "Wells Fargo       19609\n",
      "Merrill Lynch     13203\n",
      "Morgan Stanley    11246\n",
      "J.P. Morgan       11034\n",
      "Edward Jones       9699\n",
      "UBS                6793\n",
      "Ameriprise         4064\n",
      "Raymond James      2805\n",
      "Goldman Sachs       274\n",
      "BlackRock           237\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the wirehouse map\n",
    "wirehouse_map = {\n",
    "    'Bank of America Merrill Lynch': 'Merrill Lynch',\n",
    "    'Wells Fargo Advisors LLC': 'Wells Fargo',\n",
    "    'Morgan Stanley &amp; Co.': 'Morgan Stanley',\n",
    "    'J.P. Morgan Securities LLC': 'J.P. Morgan',\n",
    "    'UBS Financial Services Inc.': 'UBS',\n",
    "    'Ameriprise Financial Services Inc.': 'Ameriprise',\n",
    "    'Raymond James Financial Services Inc.': 'Raymond James',\n",
    "    'Goldman Sachs &amp; Co. LLC': 'Goldman Sachs',\n",
    "    'UBS': 'UBS',\n",
    "    'Morgan Stanley': 'Morgan Stanley',\n",
    "    'Wells Fargo Advisors': 'Wells Fargo',\n",
    "    'Wells Fargo': 'Wells Fargo',\n",
    "    'Ameriprise Financial Services': 'Ameriprise',\n",
    "    'Goldman Sachs International': 'Goldman Sachs',\n",
    "    'Raymond James Financial Services Advisors, Inc': 'Raymond James',\n",
    "    'Raymond James &amp; Associates Inc.': 'Raymond James',\n",
    "    'J.P. Morgan &amp; Co.': 'J.P. Morgan',\n",
    "    'BlackRock': 'BlackRock',\n",
    "    'Edward D. Jones': 'Edward Jones',\n",
    "    'BlackRock Alternative Investors': 'BlackRock',\n",
    "    'BlackRock Alternative Investors (BAI)': 'BlackRock'\n",
    "}\n",
    "\n",
    "# Create a normalized version of the wirehouse map for matching\n",
    "normalized_map = {k.lower(): v for k, v in wirehouse_map.items()}\n",
    "\n",
    "# Fuzzy match function using normalized names\n",
    "def match_wirehouse(name):\n",
    "    normalized_name = name.strip().lower()\n",
    "    matches = difflib.get_close_matches(normalized_name, normalized_map.keys(), n=1, cutoff=0.6)\n",
    "    return normalized_map[matches[0]] if matches else None\n",
    "\n",
    "# Apply fuzzy matching without altering the original 'Account Name'\n",
    "dakota_df['wirehouse_group'] = dakota_df['Account Name'].apply(match_wirehouse)\n",
    "\n",
    "# Count and display results\n",
    "print(f\"Total number of account entries: {dakota_df['Account Name'].count()}\")\n",
    "print(\"\\nWirehouse group counts:\")\n",
    "print(dakota_df['wirehouse_group'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2edcc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Account Totals:\n",
      "Total accounts at start: 167025\n",
      "Wirehouses removed: 78964\n",
      "Total accounts left: 88061\n"
     ]
    }
   ],
   "source": [
    "# Total accounts at start\n",
    "total_accounts_start = len(dakota_df)\n",
    "\n",
    "# Split into wirehouse and non-wirehouse groups\n",
    "dakota_wirehouses = dakota_df[dakota_df['wirehouse_group'].notna()].copy()\n",
    "dakota_non_wirehouse_df = dakota_df[dakota_df['wirehouse_group'].isna()].copy()\n",
    "\n",
    "# Count wirehouses removed and accounts left\n",
    "wirehouses_removed = len(dakota_wirehouses)\n",
    "total_accounts_left = len(dakota_non_wirehouse_df)\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the wirehouse and non-wirehouse datasets with new names\n",
    "dakota_wirehouses.to_excel(os.path.join(output_dir, \"Wirehouse_Accounts.xlsx\"), index=False)\n",
    "dakota_non_wirehouse_df.to_excel(os.path.join(output_dir, \"Non_Wirehouse_Accounts.xlsx\"), index=False)\n",
    "\n",
    "# Summary stats\n",
    "num_unique_emails = dakota_non_wirehouse_df['Email'].nunique()\n",
    "num_unique_accounts = dakota_non_wirehouse_df['Account Name'].nunique()\n",
    "num_duplicate_emails = dakota_non_wirehouse_df['Email'].duplicated().sum()\n",
    "\n",
    "print(\"\\nAccount Totals:\")\n",
    "print(f\"Total accounts at start: {total_accounts_start}\")\n",
    "print(f\"Wirehouses removed: {wirehouses_removed}\")\n",
    "print(f\"Total accounts left: {total_accounts_left}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7beb6",
   "metadata": {},
   "source": [
    "# Phase 2: Dupes and Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93ad8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Total entries: 88061\n",
      "Blank emails: 10844\n",
      "Duplicate emails (non-blank): 1052\n",
      "Duplicate names: 5092\n",
      "Cleaned entries: 71073\n"
     ]
    }
   ],
   "source": [
    "# Total entries\n",
    "total_entries = len(dakota_non_wirehouse_df)\n",
    "\n",
    "# Create Full Name column\n",
    "dakota_non_wirehouse_df['Full Name'] = (\n",
    "    dakota_non_wirehouse_df['First Name'].astype(str).str.strip() + ' ' +\n",
    "    dakota_non_wirehouse_df['Last Name'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "# Step 1: Identify blank emails\n",
    "dakota_blanks = dakota_non_wirehouse_df[\n",
    "    dakota_non_wirehouse_df['Email'].isna() |\n",
    "    (dakota_non_wirehouse_df['Email'].astype(str).str.strip() == '')\n",
    "]\n",
    "\n",
    "# Remove blanks from main dataset\n",
    "remaining_df = dakota_non_wirehouse_df.drop(dakota_blanks.index)\n",
    "\n",
    "# Step 2: Identify duplicate emails (excluding blanks)\n",
    "non_blank_df = remaining_df[\n",
    "    ~remaining_df['Email'].isna() &\n",
    "    (remaining_df['Email'].astype(str).str.strip() != '')\n",
    "]\n",
    "dakota_dupes = non_blank_df[non_blank_df.duplicated(subset='Email', keep=False)]\n",
    "dakota_dupes = dakota_dupes.sort_values(by='Email')\n",
    "\n",
    "# Remove duplicate emails from main dataset\n",
    "remaining_df = remaining_df.drop(dakota_dupes.index)\n",
    "\n",
    "# Step 3: Identify duplicate names (excluding blanks and duplicate emails)\n",
    "dakota_name_dupes = remaining_df[\n",
    "    remaining_df.duplicated(subset='Full Name', keep=False)\n",
    "].sort_values(by='Full Name')\n",
    "\n",
    "# Final cleaned dataset (not in any of the above categories)\n",
    "cleaned_df = remaining_df.drop(dakota_name_dupes.index)\n",
    "\n",
    "# Counts\n",
    "num_blank = len(dakota_blanks)\n",
    "num_duplicates = len(dakota_dupes)\n",
    "num_name_duplicates = len(dakota_name_dupes)\n",
    "num_cleaned = len(cleaned_df)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"Blank emails: {num_blank}\")\n",
    "print(f\"Duplicate emails (non-blank): {num_duplicates}\")\n",
    "print(f\"Duplicate names: {num_name_duplicates}\")\n",
    "print(f\"Cleaned entries: {num_cleaned}\")\n",
    "\n",
    "# Save outputs\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dakota_blanks.to_excel(os.path.join(output_dir, \"Dakota_Blanks.xlsx\"), index=False)\n",
    "dakota_dupes.to_excel(os.path.join(output_dir, \"Dakota_Duplicates.xlsx\"), index=False)\n",
    "dakota_name_dupes.to_excel(os.path.join(output_dir, \"Dakota_DuplicateNames.xlsx\"), index=False)\n",
    "cleaned_df.to_excel(os.path.join(output_dir, \"Dakota_Cleaned.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf1141",
   "metadata": {},
   "source": [
    "# Dakota AUM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d51338ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking AUM Data for dupes\n",
    "matched_accounts[\"Provided Account ID\"] = matched_accounts[\"Provided Account ID\"].str.strip().str.upper()\n",
    "matched_contacts[\"Provided Salesforce Contact Account Record ID\"] = matched_contacts[\"Provided Salesforce Contact Account Record ID\"].str.strip().str.upper()\n",
    "print(matched_contacts[\"Provided Salesforce Contact Account Record ID\"].isnull().sum())\n",
    "print(matched_accounts[\"Provided Account ID\"].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be13b4",
   "metadata": {},
   "source": [
    "### Matching AUM Data\n",
    "- Output only matched account IDs across 4 files\n",
    "\n",
    "    - matched contacts\n",
    "    - matched accounts\n",
    "    - unmatched contacts\n",
    "    - unmatched accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77ac4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched accounts: 5391\n",
      "Matched contacts: 18374\n",
      "Unmatched accounts: 34435\n",
      "Unmatched contacts: 167025\n",
      "\n",
      "Matched contacts: 16927\n",
      "Match rate: 92.12%\n",
      "\n",
      "Unmatched contacts matched to accounts: 69419\n",
      "Unmatched match rate: 41.56%\n",
      "\n",
      "Total matched contacts: 16927\n",
      "\n",
      "NaN count per column:\n",
      "Provided Account ID                                  0\n",
      "Provided Account Name                                0\n",
      "Provided Firm ID                                 16092\n",
      "Provided BillingStreet                           12808\n",
      "Provided Billing City                            11098\n",
      "                                                 ...  \n",
      "Dakota Last Modified Date/Time (DATALOADER)_y        0\n",
      "Dakota Created Date/Time (DATALOADER)_y              0\n",
      "Dakota Status_y                                      0\n",
      "Matching Criteria_y                                  0\n",
      "Duplicates_y                                         0\n",
      "Length: 122, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "matched_accounts_path = \"../data/prospect/matched_accounts.xlsx\"\n",
    "matched_contacts_path = \"../data/prospect/matched_contacts.xlsx\"\n",
    "unmatched_accounts_path = \"../data/prospect/unmatched_accounts.xlsx\"\n",
    "unmatched_contacts_path = \"../data/prospect/unmatched_contacts.xlsx\"\n",
    "\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load matched datasets\n",
    "matched_accounts = pd.read_excel(matched_accounts_path, engine='openpyxl')\n",
    "matched_contacts = pd.read_excel(matched_contacts_path, engine='openpyxl')\n",
    "\n",
    "# Count rows before merge\n",
    "print(f\"Matched accounts: {len(matched_accounts)}\")\n",
    "print(f\"Matched contacts: {len(matched_contacts)}\")\n",
    "\n",
    "# Merge matched accounts and contacts\n",
    "aum_matched = pd.merge(\n",
    "    matched_accounts,\n",
    "    matched_contacts,\n",
    "    left_on=\"Provided Account ID\",\n",
    "    right_on=\"Provided Salesforce Contact Account Record ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save matched results\n",
    "aum_matched.to_excel(os.path.join(output_dir, \"AUM_matched.xlsx\"), index=False)\n",
    "\n",
    "# Load unmatched datasets\n",
    "unmatched_accounts = pd.read_excel(unmatched_accounts_path, engine='openpyxl')\n",
    "unmatched_contacts = pd.read_excel(unmatched_contacts_path, engine='openpyxl')\n",
    "\n",
    "# Count rows before merge\n",
    "print(f\"Unmatched accounts: {len(unmatched_accounts)}\")\n",
    "print(f\"Unmatched contacts: {len(unmatched_contacts)}\")\n",
    "\n",
    "# Attempt to match unmatched contacts to unmatched accounts\n",
    "aum_unmatched = pd.merge(\n",
    "    unmatched_accounts,\n",
    "    unmatched_contacts,\n",
    "    on=\"Account ID (Case Safe)\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save unmatched results\n",
    "aum_unmatched.to_excel(os.path.join(output_dir, \"AUM_unmatched.xlsx\"), index=False)\n",
    "\n",
    "# Print match statistics\n",
    "match_percentage = (len(aum_matched) / len(matched_contacts)) * 100\n",
    "print(f\"\\nMatched contacts: {len(aum_matched)}\")\n",
    "print(f\"Match rate: {match_percentage:.2f}%\")\n",
    "\n",
    "unmatch_percentage = (len(aum_unmatched) / len(unmatched_contacts)) * 100\n",
    "print(f\"\\nUnmatched contacts matched to accounts: {len(aum_unmatched)}\")\n",
    "print(f\"Unmatched match rate: {unmatch_percentage:.2f}%\")\n",
    "\n",
    "# Count total contacts\n",
    "total_contacts = len(aum_matched)\n",
    "print(f\"\\nTotal matched contacts: {total_contacts}\")\n",
    "\n",
    "# Count NaN values per column\n",
    "nan_summary = aum_matched.isna().sum()\n",
    "print(\"\\nNaN count per column:\")\n",
    "print(nan_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1166ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Provided Account ID', 'Provided Account Name', 'Provided Firm ID', 'Provided BillingStreet', 'Provided Billing City', 'Provided BillingCountry', 'Provided BillingPostalCode', 'Provided Website', 'Provided CRD', 'Dakota Account ID (Case Safe)_x', 'Dakota Account Name_x', 'Dakota Type', 'Dakota AUM', 'Dakota Metro Area', 'Dakota Parent Account', 'Dakota Website', 'Dakota Phone_x', 'Dakota CRD#', 'Dakota Billing Street', 'Dakota Billing City', 'Dakota Billing State/Province', 'Dakota Billing Zip/Postal Code', 'Dakota Billing Country', 'Dakota Description', 'Dakota Platform Description', 'Dakota Research Team Overview', 'Dakota Opportunity Description', 'Dakota Custodian(s)', 'Dakota UHNW Division', 'Dakota TAMP', 'Dakota Sub-Advised MF Family', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models', 'Dakota Client Base', 'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG', 'Dakota Preferred Investment Vehicle', 'Dakota Mutual Fund Usage', 'Dakota LP Usage', 'Dakota Separate Account Usage', 'Dakota RIC Usage', 'Dakota UMA Usage', 'Dakota ETF Usage', 'Dakota CIT Usage', 'Dakota UCITS Usage', 'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF', 'Dakota General Consultant', 'Dakota General Consultant 2', 'Dakota Hedge Fund Consultant', 'Dakota Real Estate Consultant', 'Dakota Private Equity Consultant', 'Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities', 'Dakota Micro Cap US Equities', 'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities', 'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds', 'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans', 'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles', 'Dakota Private Equity', 'Dakota Private Credit', 'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives', 'Dakota Real Assets', 'Dakota Venture Capital', 'Dakota Alternative Platform Description', 'Dakota Last Modified Date/Time', 'Dakota Created Date/Time', 'Dakota Last Modified Date/Time (DATALOADER)_x', 'Dakota Created Date/Time (DATALOADER)_x', 'Dakota Status_x', 'Matching Criteria_x', 'Duplicates_x', 'Provided Contact ID', 'Provided Salesforce Contact Account Record ID', 'Provided First Name', 'Provided Last Name', 'Provided Email', 'Provided Phone', 'Provided MailingStreet', 'Provided MailingCity', 'Provided MailingState', 'Provided MailingCountry', 'Provided MailingPostalCode', 'Dakota ContactID', 'Dakota First Name', 'Dakota Last Name', 'Dakota Email', 'Dakota Account ID (Case Safe)_y', 'Dakota Account Name_y', 'Dakota Metro Area: Metro Area Name', 'Dakota Phone_y', 'Dakota Title', 'Dakota Contact Type', 'Dakota Asset Class Coverage', 'Dakota Mailing Street', 'Dakota Mailing City', 'Dakota Mailing State/Province', 'Dakota Mailing Zip/Postal Code', 'Dakota Mailing Country', 'Dakota Biography', 'Dakota CRD #', 'Dakota Last Modified DateTime', 'Dakota Created DateTime', 'Dakota Last Modified Date/Time (DATALOADER)_y', 'Dakota Created Date/Time (DATALOADER)_y', 'Dakota Status_y', 'Matching Criteria_y', 'Duplicates_y']\n"
     ]
    }
   ],
   "source": [
    "print(aum_matched.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a49f2",
   "metadata": {},
   "source": [
    "Final Dataset for AUM Structure:\n",
    "\n",
    "Provided Account ID | Provided Account Name | Provided Firm ID | Provided BillingStreet | Provided Billing City | Provided BillingCountry | Provided BillingPostalCode | Provided Website | Provided CRD | Dakota Account ID (Case Safe)_x | Dakota Account Name_x\t| Dakota Type | Dakota AUM | Dakota Metro Area | Dakota Parent Account | Dakota Website\tDakota | Phone_x | Dakota CRD# | Dakota Billing Street | Dakota Billing City | Dakota Billing State/Province | Dakota Billing Zip/Postal Code | Dakota Billing Country | Dakota Description | Dakota Platform Description | Dakota Research Team Overview | Dakota Opportunity Description | Dakota Custodian(s) | Dakota UHNW Division | Dakota TAMP | Dakota Sub-Advised MF Family | Dakota Select Lists | Dakota OCIO Business | Dakota Models | Dakota Client Base | Dakota Emerging Manager Program | Dakota Invests in Impact, SRI or ESG | Dakota Preferred Investment Vehicle | Dakota Mutual Fund Usage | Dakota LP Usage | Dakota Separate Account Usage | Dakota RIC Usage | Dakota UMA Usage | Dakota ETF Usage | Dakota CIT Usage | Dakota UCITS Usage | Dakota Hedge FOF | Dakota Real Estate FOF | Dakota Private Equity FOF\tDakota General Consultant | Dakota General Consultant 2\t| Dakota Hedge Fund Consultant | Dakota Real Estate Consultant | Dakota Private Equity Consultant | Dakota Small Cap Equities | Dakota Mid Cap Equities\tDakota Large Cap Equities | Dakota Micro Cap US Equities | Dakota International Equities |\tDakota Emerging Market Equities | Dakota Global Equities | Dakota Municipal Bonds | Dakota Core Bonds | Dakota Emerging Market Bonds | Dakota High Yield Bonds | Dakota Government Bonds | Dakota Bank Loans | Dakota Unconstrained\tDakota MBS | Dakota CMBS | Dakota Convertibles | Dakota Private Equity | Dakota Private Credit | Dakota Hedge Funds | Dakota Private Real Estate | Dakota Liquid Alternatives | Dakota Real Assets | Dakota Venture Capital | Dakota Alternative Platform Description | Dakota Last Modified Date/Time | Dakota Created Date/Time | Dakota Last Modified Date/Time (DATALOADER)_x | Dakota Created Date/Time (DATALOADER)_x | Dakota Status_x | Matching Criteria_x | Duplicates_x | Provided Contact ID | Provided Salesforce Contact Account Record ID | Provided First Name | Provided Last Name | Provided Email | Provided Phone | Provided MailingStreet | Provided MailingCity | Provided MailingState | Provided MailingCountry | Provided MailingPostalCode | Dakota ContactID |\tDakota First Name | Dakota Last Name | Dakota Email | Dakota Account ID (Case Safe)_y | Dakota Account Name_y | Dakota Metro Area: Metro Area Name | Dakota Phone_y | Dakota Title | Dakota Contact Type | Dakota Asset Class Coverage | Dakota Mailing Street | Dakota Mailing City | Dakota Mailing State/Province | Dakota Mailing Zip/Postal Code | Dakota Mailing Country | Dakota Biography | Dakota CRD # | Dakota Last Modified DateTime | Dakota Created DateTime | Dakota Last Modified Date/Time (DATALOADER)_y | Dakota Created Date/Time (DATALOADER)_y | Dakota Status_y | Matching Criteria_y | Duplicates_y | Converted?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053da8e",
   "metadata": {},
   "source": [
    "# Matching win list to labeled dataset\n",
    "- Of the historical wins, what % can be mapped to labels?\n",
    "- 'Provided Account Name' - 'Potential Client'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f421b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Yes/No columns to binary: ['Provided CRD', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models', 'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG', 'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF', 'Dakota Micro Cap US Equities', 'Dakota Private Equity', 'Dakota Private Credit', 'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives', 'Dakota Real Assets', 'Dakota Venture Capital']\n",
      "Converted strategy columns: ['Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities', 'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities', 'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds', 'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans', 'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles', 'Dakota Status_x', 'Dakota Status_y']\n",
      "Unmapped values found in strategy columns:\n",
      "Dakota Small Cap Equities: ['0.0', '1.0', '3.0', '2.0']\n",
      "Dakota Mid Cap Equities: ['0.0', '1.0', '2.0', '3.0']\n",
      "Dakota Large Cap Equities: ['0.0', '2.0', '3.0', '1.0']\n",
      "Dakota International Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Emerging Market Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Global Equities: ['1.0', '0.0', '3.0', '2.0']\n",
      "Dakota Municipal Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Core Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Emerging Market Bonds: ['0.0', '1.0', '3.0']\n",
      "Dakota High Yield Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Government Bonds: ['1.0', '0.0', '3.0']\n",
      "Dakota Bank Loans: ['1.0', '0.0', '3.0']\n",
      "Dakota Unconstrained: ['1.0', '0.0', '3.0']\n",
      "Dakota MBS: ['1.0', '0.0', '3.0']\n",
      "Dakota CMBS: ['1.0', '0.0', '3.0']\n",
      "Dakota Convertibles: ['1.0', '0.0', '3.0']\n",
      "Dakota Status_x: ['1']\n",
      "Dakota Status_y: ['1']\n",
      "Unmapped values in 'Dakota Preferred Investment Vehicle': ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0']\n",
      "Unmapped values in usage columns:\n",
      "Dakota Mutual Fund Usage: ['3.0', '2.0', '1.0', '0.0']\n",
      "Dakota LP Usage: ['2.0', '3.0', '1.0', '0.0']\n",
      "Dakota Separate Account Usage: ['3.0', '2.0', '1.0', '0.0']\n",
      "Dakota RIC Usage: ['1.0', '2.0', '3.0', '0.0']\n",
      "Dakota UMA Usage: ['3.0', '2.0', '0.0', '1.0']\n",
      "Dakota ETF Usage: ['3.0', '2.0', '0.0', '1.0']\n",
      "Dakota CIT Usage: ['1.0', '2.0', '0.0', '3.0']\n",
      "Dakota UCITS Usage: ['0.0', '2.0', '1.0', '3.0']\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled AUM dataset\n",
    "aum_labeled = pd.read_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Convert Yes/No columns to binary 1/0\n",
    "yes_no_columns = [\n",
    "    'Provided CRD', 'Dakota Select Lists', 'Dakota OCIO Business', 'Dakota Models',\n",
    "    'Dakota Emerging Manager Program', 'Dakota Invests in Impact, SRI or ESG',\n",
    "    'Dakota Hedge FOF', 'Dakota Real Estate FOF', 'Dakota Private Equity FOF',\n",
    "    'Dakota Micro Cap US Equities', 'Dakota Private Equity', 'Dakota Private Credit',\n",
    "    'Dakota Hedge Funds', 'Dakota Private Real Estate', 'Dakota Liquid Alternatives',\n",
    "    'Dakota Real Assets', 'Dakota Venture Capital'\n",
    "]\n",
    "aum_labeled[yes_no_columns] = aum_labeled[yes_no_columns].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Define strategy mapping\n",
    "strategy_map = {\n",
    "    'Active': 1,\n",
    "    'Passive': 2,\n",
    "    'Active/Passive': 0,\n",
    "    'In-House': 3,\n",
    "    'Yes': 1,\n",
    "    'No': 0\n",
    "}\n",
    "\n",
    "# Strategy columns to convert\n",
    "strategy_columns = [\n",
    "    'Dakota Small Cap Equities', 'Dakota Mid Cap Equities', 'Dakota Large Cap Equities',\n",
    "    'Dakota International Equities', 'Dakota Emerging Market Equities', 'Dakota Global Equities',\n",
    "    'Dakota Municipal Bonds', 'Dakota Core Bonds', 'Dakota Emerging Market Bonds',\n",
    "    'Dakota High Yield Bonds', 'Dakota Government Bonds', 'Dakota Bank Loans',\n",
    "    'Dakota Unconstrained', 'Dakota MBS', 'Dakota CMBS', 'Dakota Convertibles',\n",
    "    'Dakota Status_x', 'Dakota Status_y'\n",
    "]\n",
    "\n",
    "# Mapping for Dakota Preferred Investment Vehicle\n",
    "vehicle_map = {\n",
    "    'Mutual Funds': 1,\n",
    "    'ETFs': 2,\n",
    "    'Separate Accounts': 3,\n",
    "    'LP': 4,\n",
    "    'Hedge Funds': 5,\n",
    "    'Subadvisory': 6\n",
    "}\n",
    "\n",
    "# Mapping for usage levels\n",
    "usage_map = {\n",
    "    'Zero': 0,\n",
    "    'Small': 1,\n",
    "    'Medium': 2,\n",
    "    'Large': 3\n",
    "}\n",
    "\n",
    "# Apply vehicle mapping\n",
    "vehicle_col = 'Dakota Preferred Investment Vehicle'\n",
    "unmapped_vehicles = aum_labeled[vehicle_col].dropna().astype(str).str.strip().unique()\n",
    "unmapped_vehicles = [val for val in unmapped_vehicles if val not in vehicle_map]\n",
    "aum_labeled[vehicle_col] = aum_labeled[vehicle_col].replace(vehicle_map)\n",
    "\n",
    "# Apply usage mapping\n",
    "usage_columns = [\n",
    "    'Dakota Mutual Fund Usage', 'Dakota LP Usage', 'Dakota Separate Account Usage',\n",
    "    'Dakota RIC Usage', 'Dakota UMA Usage', 'Dakota ETF Usage',\n",
    "    'Dakota CIT Usage', 'Dakota UCITS Usage'\n",
    "]\n",
    "\n",
    "unmapped_usage = {}\n",
    "for col in usage_columns:\n",
    "    unique_vals = aum_labeled[col].dropna().astype(str).str.strip().unique()\n",
    "    unmapped = [val for val in unique_vals if val not in usage_map]\n",
    "    if unmapped:\n",
    "        unmapped_usage[col] = unmapped\n",
    "    aum_labeled[col] = aum_labeled[col].replace(usage_map)\n",
    "\n",
    "# Save the updated dataset\n",
    "aum_labeled.to_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", index=False)\n",
    "\n",
    "# Track unmapped values\n",
    "unmapped_values = {}\n",
    "\n",
    "# Apply strategy mapping and collect unmapped values\n",
    "for col in strategy_columns:\n",
    "    unique_vals = aum_labeled[col].dropna().astype(str).str.strip().unique()\n",
    "    unmapped = [val for val in unique_vals if val not in strategy_map]\n",
    "    if unmapped:\n",
    "        unmapped_values[col] = unmapped\n",
    "    aum_labeled[col] = aum_labeled[col].replace(strategy_map)\n",
    "\n",
    "# Save the updated dataset\n",
    "aum_labeled.to_excel(\"../data/dakota_salesforce_sheets/AUM_labeled.xlsx\", index=False)\n",
    "\n",
    "# Output summary\n",
    "print(\"Converted Yes/No columns to binary:\", yes_no_columns)\n",
    "print(\"Converted strategy columns:\", strategy_columns)\n",
    "print(\"Unmapped values found in strategy columns:\")\n",
    "for col, vals in unmapped_values.items():\n",
    "    print(f\"{col}: {vals}\")\n",
    "\n",
    "print(\"Unmapped values in 'Dakota Preferred Investment Vehicle':\", unmapped_vehicles)\n",
    "print(\"Unmapped values in usage columns:\")\n",
    "for col, vals in unmapped_usage.items():\n",
    "    print(f\"{col}: {vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5a83",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebb556",
   "metadata": {},
   "source": [
    "# Salesforce Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "322a86bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce Summary:\n",
      "Total entries: 48678\n",
      "Blank emails: 9864\n",
      "Duplicate emails (non-blank): 17\n",
      "Duplicate names: 2018\n",
      "Cleaned entries: 36779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Country Standardization ---\n",
    "def standardize_country(country, state):\n",
    "    if pd.isnull(country):\n",
    "        country = ''\n",
    "    country_upper = str(country).strip().upper()\n",
    "    if country_upper in ['UNITED STATES', 'U.S.', 'USA', 'US']:\n",
    "        return 'United States'\n",
    "    if pd.notnull(state) and str(state).strip() != '':\n",
    "        return 'United States'\n",
    "    return country\n",
    "\n",
    "country_column_candidates = [\n",
    "    'Mailing Country', 'MailingCountry', 'Billing Country', 'BillingCountry',\n",
    "    'Provided BillingCountry', 'Dakota Billing Country', 'Provided MailingCountry', 'Dakota Mailing Country'\n",
    "]\n",
    "state_column_candidates = [\n",
    "    'Mailing State/Province', 'Mailing State', 'Billing State/Province', 'Billing State'\n",
    "]\n",
    "\n",
    "def find_column(df, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def apply_country_standardization(df):\n",
    "    country_col = find_column(df, country_column_candidates)\n",
    "    state_col = find_column(df, state_column_candidates)\n",
    "    if country_col:\n",
    "        df['Mailing Country'] = df.apply(\n",
    "            lambda row: standardize_country(row.get(country_col), row.get(state_col)),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "# --- Load and Clean Salesforce Dataset ---\n",
    "salesforce_df = pd.read_excel(\"../data/all_contacts/Salesforce_dataset.xlsx\", engine=\"openpyxl\", dtype=str)\n",
    "\n",
    "# Apply country standardization\n",
    "apply_country_standardization(salesforce_df)\n",
    "\n",
    "# Create Full Name column\n",
    "salesforce_df['Full Name'] = (\n",
    "    salesforce_df['First Name'].astype(str).str.strip() + ' ' +\n",
    "    salesforce_df['Last Name'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "# Total entries (including blanks)\n",
    "total_entries = len(salesforce_df)\n",
    "\n",
    "# Step 1: Identify blank emails\n",
    "sf_blanks = salesforce_df[\n",
    "    salesforce_df['Email'].isna() |\n",
    "    (salesforce_df['Email'].astype(str).str.strip() == '')\n",
    "]\n",
    "\n",
    "# Remove blanks from main dataset\n",
    "remaining_df = salesforce_df.drop(sf_blanks.index)\n",
    "\n",
    "# Step 2: Identify duplicate emails (excluding blanks)\n",
    "non_blank_df = remaining_df[\n",
    "    ~remaining_df['Email'].isna() &\n",
    "    (remaining_df['Email'].astype(str).str.strip() != '')\n",
    "]\n",
    "sf_dupes = non_blank_df[non_blank_df.duplicated(subset='Email', keep=False)]\n",
    "sf_dupes = sf_dupes.sort_values(by='Email')\n",
    "\n",
    "# Remove duplicate emails from main dataset\n",
    "remaining_df = remaining_df.drop(sf_dupes.index)\n",
    "\n",
    "# Step 3: Identify duplicate names (excluding blanks and duplicate emails)\n",
    "sf_name_dupes = remaining_df[\n",
    "    remaining_df.duplicated(subset='Full Name', keep=False)\n",
    "].sort_values(by='Full Name')\n",
    "\n",
    "# Final cleaned dataset (not in any of the above categories)\n",
    "sf_cleaned = remaining_df.drop(sf_name_dupes.index)\n",
    "\n",
    "# Counts\n",
    "num_blank = len(sf_blanks)\n",
    "num_duplicates = len(sf_dupes)\n",
    "num_name_duplicates = len(sf_name_dupes)\n",
    "num_cleaned = len(sf_cleaned)\n",
    "\n",
    "# Print summary\n",
    "print(\"Salesforce Summary:\")\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"Blank emails: {num_blank}\")\n",
    "print(f\"Duplicate emails (non-blank): {num_duplicates}\")\n",
    "print(f\"Duplicate names: {num_name_duplicates}\")\n",
    "print(f\"Cleaned entries: {num_cleaned}\")\n",
    "\n",
    "# Save outputs to the same Dakota folder with Salesforce-specific names\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sf_blanks.to_excel(os.path.join(output_dir, \"Salesforce_Blanks.xlsx\"), index=False)\n",
    "sf_dupes.to_excel(os.path.join(output_dir, \"Salesforce_Duplicates.xlsx\"), index=False)\n",
    "sf_name_dupes.to_excel(os.path.join(output_dir, \"Salesforce_DuplicateNames.xlsx\"), index=False)\n",
    "sf_cleaned.to_excel(os.path.join(output_dir, \"Salesforce_Cleaned.xlsx\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56195c",
   "metadata": {},
   "source": [
    "# Non Wirehouse Dakota X Salesforce Match\n",
    "- Compare and contrast\n",
    "- Match with Dakota overriding uncertain salesforce bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6bd45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overlap and Merge Summary ===\n",
      "Total Dakota entries: 71073\n",
      "Total Salesforce entries: 36779\n",
      "Overlap by Email: 298\n",
      "Overlap by Name only: 2698\n",
      "Dakota contacts to merge: 68079\n",
      "Salesforce contacts to merge: 33790\n",
      "Total contacts in final merge list: 101869\n",
      "\n",
      "Top Email Domains in Merge List:\n",
      "Email\n",
      "lpl.com              15005\n",
      "rbc.com               2544\n",
      "ubs.com               2139\n",
      "stifel.com            2044\n",
      "morganstanley.com     1748\n",
      "ml.com                1457\n",
      "commonwealth.com      1312\n",
      "rwbaird.com           1306\n",
      "nm.com                1255\n",
      "schwab.com            1205\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top Titles in Merge List:\n",
      "Title\n",
      "Financial Advisor           27316\n",
      "Managing Director            1651\n",
      "Portfolio Manager            1211\n",
      "Investment Analyst           1182\n",
      "Chief Investment Officer     1108\n",
      "Wealth Advisor               1057\n",
      "Partner                       947\n",
      "Chief Financial Officer       699\n",
      "President                     694\n",
      "Investment Advisor            693\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned datasets\n",
    "dakota_df = pd.read_excel(\"../data/dakota_salesforce_sheets/Dakota_Cleaned.xlsx\", engine=\"openpyxl\")\n",
    "salesforce_df = pd.read_excel(\"../data/dakota_salesforce_sheets/Salesforce_Cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Normalize email and full name for matching\n",
    "dakota_df['Email_norm'] = dakota_df['Email'].astype(str).str.strip().str.lower()\n",
    "salesforce_df['Email_norm'] = salesforce_df['Email'].astype(str).str.strip().str.lower()\n",
    "\n",
    "dakota_df['Full Name_norm'] = dakota_df['First Name'].astype(str).str.strip().str.lower() + ' ' + dakota_df['Last Name'].astype(str).str.strip().str.lower()\n",
    "salesforce_df['Full Name_norm'] = salesforce_df['First Name'].astype(str).str.strip().str.lower() + ' ' + salesforce_df['Last Name'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Identify overlaps by email\n",
    "email_overlap = pd.merge(\n",
    "    dakota_df, salesforce_df,\n",
    "    on='Email_norm',\n",
    "    suffixes=('_dakota', '_salesforce')\n",
    ")\n",
    "\n",
    "# Identify overlaps by full name (excluding those already matched by email)\n",
    "email_overlap_keys = set(email_overlap['Email_norm'])\n",
    "name_overlap = pd.merge(\n",
    "    dakota_df, salesforce_df,\n",
    "    on='Full Name_norm',\n",
    "    suffixes=('_dakota', '_salesforce')\n",
    ")\n",
    "name_overlap = name_overlap[~name_overlap['Email_norm_dakota'].isin(email_overlap_keys)]\n",
    "\n",
    "# Identify non-overlapping Dakota entries\n",
    "dakota_non_overlap = dakota_df[\n",
    "    ~dakota_df['Email_norm'].isin(email_overlap['Email_norm']) &\n",
    "    ~dakota_df['Full Name_norm'].isin(name_overlap['Full Name_norm'])\n",
    "]\n",
    "\n",
    "# Identify non-overlapping Salesforce entries\n",
    "salesforce_non_overlap = salesforce_df[\n",
    "    ~salesforce_df['Email_norm'].isin(email_overlap['Email_norm']) &\n",
    "    ~salesforce_df['Full Name_norm'].isin(name_overlap['Full Name_norm'])\n",
    "]\n",
    "\n",
    "# Combine non-overlapping entries into a unified merge list\n",
    "contacts_merge_list = pd.concat([dakota_non_overlap, salesforce_non_overlap], ignore_index=True)\n",
    "\n",
    "# Print counts\n",
    "print(\"=== Overlap and Merge Summary ===\")\n",
    "print(f\"Total Dakota entries: {len(dakota_df)}\")\n",
    "print(f\"Total Salesforce entries: {len(salesforce_df)}\")\n",
    "print(f\"Overlap by Email: {len(email_overlap)}\")\n",
    "print(f\"Overlap by Name only: {len(name_overlap)}\")\n",
    "print(f\"Dakota contacts to merge: {len(dakota_non_overlap)}\")\n",
    "print(f\"Salesforce contacts to merge: {len(salesforce_non_overlap)}\")\n",
    "print(f\"Total contacts in final merge list: {len(contacts_merge_list)}\")\n",
    "\n",
    "# Optional: Value counts for verification\n",
    "print(\"\\nTop Email Domains in Merge List:\")\n",
    "print(contacts_merge_list['Email'].dropna().apply(lambda x: x.split('@')[-1]).value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop Titles in Merge List:\")\n",
    "print(contacts_merge_list['Title'].value_counts().head(10))\n",
    "\n",
    "# Save outputs\n",
    "output_dir = \"../data/dakota_salesforce_sheets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "email_overlap.to_excel(os.path.join(output_dir, \"Overlap_By_Email.xlsx\"), index=False)\n",
    "name_overlap.to_excel(os.path.join(output_dir, \"Overlap_By_Name.xlsx\"), index=False)\n",
    "dakota_non_overlap.to_excel(os.path.join(output_dir, \"Dakota_Contacts_Merge_List.xlsx\"), index=False)\n",
    "contacts_merge_list.to_excel(os.path.join(output_dir, \"Final_Contacts_Merge_List.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905f001",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d865132",
   "metadata": {},
   "source": [
    "# Accounts Clean + Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0551dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarkBogorad\\AppData\\Local\\Temp\\ipykernel_31164\\3927566952.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_df['Mailing Country'] = unique_df.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dakota accounts: 34284\n",
      "Total Salesforce accounts: 12395\n",
      "Number of overlapping accounts: 99\n",
      "Number of unique accounts to Dakota: 34185\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define standardization function\n",
    "def standardize_country(country, state):\n",
    "    if pd.isnull(country):\n",
    "        country = ''\n",
    "    country_upper = str(country).strip().upper()\n",
    "    if country_upper in ['UNITED STATES', 'U.S.', 'USA', 'US']:\n",
    "        return 'United States'\n",
    "    if pd.notnull(state) and str(state).strip() != '':\n",
    "        return 'United States'\n",
    "    return country\n",
    "\n",
    "# List of possible column names for country and state\n",
    "country_column_candidates = [\n",
    "    'Mailing Country', 'MailingCountry', 'Billing Country', 'BillingCountry',\n",
    "    'Provided BillingCountry', 'Dakota Billing Country', 'Provided MailingCountry', 'Dakota Mailing Country'\n",
    "]\n",
    "state_column_candidates = [\n",
    "    'Mailing State/Province', 'Mailing State', 'Billing State/Province', 'Billing State'\n",
    "]\n",
    "\n",
    "# Function to find the first matching column from a list of candidates\n",
    "def find_column(df, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# Load Dakota unmatched accounts\n",
    "dakota_df = pd.read_excel(\"../data/prospect/unmatched_prod_accts.xlsx\")\n",
    "salesforce_df = pd.read_excel(\"../data/dakota_salesforce_sheets/SF_Accounts.xlsx\")\n",
    "\n",
    "# Normalize account names\n",
    "dakota_df['normalized_name'] = dakota_df['Account Name'].str.strip().str.lower()\n",
    "salesforce_df['normalized_name'] = salesforce_df['Name'].str.strip().str.lower()\n",
    "\n",
    "# Compare sets\n",
    "dakota_accounts = set(dakota_df['normalized_name'].dropna().unique())\n",
    "salesforce_accounts = set(salesforce_df['normalized_name'].dropna().unique())\n",
    "unique_to_dakota = dakota_accounts - salesforce_accounts\n",
    "\n",
    "# Filter Dakota data to only include unique accounts\n",
    "unique_df = dakota_df[dakota_df['normalized_name'].isin(unique_to_dakota)]\n",
    "\n",
    "# Apply country standardization\n",
    "country_col = find_column(unique_df, country_column_candidates)\n",
    "state_col = find_column(unique_df, state_column_candidates)\n",
    "if country_col:\n",
    "    unique_df['Mailing Country'] = unique_df.apply(\n",
    "        lambda row: standardize_country(row.get(country_col), row.get(state_col)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Save to file\n",
    "unique_df.to_excel(\"../data/all_contacts/unique_accounts.xlsx\", index=False)\n",
    "\n",
    "# Print counts\n",
    "print(f\"Total Dakota accounts: {len(dakota_accounts)}\")\n",
    "print(f\"Total Salesforce accounts: {len(salesforce_accounts)}\")\n",
    "print(f\"Number of overlapping accounts: {len(dakota_accounts & salesforce_accounts)}\")\n",
    "print(f\"Number of unique accounts to Dakota: {len(unique_to_dakota)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdf2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
